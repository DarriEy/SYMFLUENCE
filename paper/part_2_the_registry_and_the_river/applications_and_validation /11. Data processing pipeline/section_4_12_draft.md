# 4.12 Data Processing Pipeline

The preceding experiments assumed that forcing data, geospatial attributes, and observational datasets were already preprocessed and available for model consumption. In practice, transforming heterogeneous raw data into model-ready inputs constitutes a substantial and error-prone portion of the modelling workflow — one that is rarely documented, difficult to reproduce, and frequently consumes more researcher effort than the simulation itself. This section evaluates SYMFLUENCE's data processing pipeline by tracing its end-to-end transformation chain through three domains spanning the full range of scales in this paper: Paradise SNOTEL (point scale, 1 GRU, 0.01 km²), Bow at Banff (watershed scale, 49 HRUs, 2,210 km²), and Iceland (regional scale, 21,474 HRUs across 6,600 GRUs, 103,000 km²). Together, these three domains span over six orders of magnitude in area and over four orders of magnitude in spatial element count, providing a comprehensive test of the pipeline's scalability.

## 4.12.1 Pipeline Architecture and Scaling

The SYMFLUENCE data processing pipeline comprises 16 stages organised as a directed acyclic graph (DAG) with 25 data-flow edges (Figure Xa). Stages fall into five categories:

- **Setup** (1 stage): project initialisation and configuration validation.
- **Geospatial processing** (5 stages): pour-point creation, domain delineation, discretisation into GRUs, attribute acquisition (DEM, soil class, land cover), and zonal statistics computation.
- **Forcing preprocessing** (6 stages): ERA5 acquisition via CDS API, EASYMORE weight generation (forcing grid × GRU intersection), weight application (matrix multiplication), variable standardisation to CFIF, elevation lapse-rate correction, and forcing file merging.
- **Observation processing** (3 stages): streamflow (WSC/IMO), snow (SNOTEL/MODIS SCA), and ET/GRACE retrieval with quality control.
- **Model setup** (1 stage): conversion from CFIF to model-specific input format.

The DAG structure enables both sequential execution (for single-machine workflows) and partial parallelism: the three observation processing stages are independent of the forcing preprocessing chain and can execute concurrently. The geospatial stages must complete before forcing weight generation can begin, but attribute acquisition and forcing acquisition can proceed in parallel once the domain boundary is defined. Data flow between stages is typed: edges carry shapefiles (for spatial boundaries), raster products (DEM, land cover, soil class), NetCDF (forcing grids, remapping weights, model inputs), CSV (observation time series), or configuration metadata. This typing enforces interface contracts between stages and enables the framework to validate outputs before passing them downstream.

Each stage is invoked independently via the SYMFLUENCE CLI (`symfluence workflow step <stage> --config <path>`), and all configuration is declarative: switching between domains requires changing only the domain coordinates, discretisation settings, and observation sources in the YAML configuration file. The same 16-stage pipeline is executed identically for Paradise (Sections 4.3, 4.10), Bow (Sections 4.2, 4.4–4.7), and Iceland (Sections 4.8, 4.9), with the framework automatically adapting data acquisition extents, remapping matrix dimensions, and observation source selection based on the configuration.

Figure Xb presents the total data volume generated by each pipeline category across the three domains, plotted against HRU count on log-log axes. The colour coding in Figure Xb matches the DAG categories in Figure Xa, linking architecture to resource consumption. Total pipeline output ranges from 12.3 MB (Paradise, 110 files) through 132.9 MB (Bow, 195 files) to 4.3 GB (Iceland, 101 files). Forcing data dominates at all scales, but the scaling rate differs by category: geospatial products (DEM, soil, land cover, shapefiles) scale with domain area, growing from 10 KB (Paradise) to 170 MB (Iceland); forcing volumes scale with both area and HRU count, with basin-averaged forcing expanding from 1.6 MB (Paradise) to 3.4 GB (Iceland) as the spatial remapping redistributes gridded data across increasingly many HRUs; observation volumes depend on the number and type of available products rather than spatial resolution, remaining at 14 MB for station streamflow regardless of domain complexity while reaching 194 MB for Iceland's combined streamflow, ET, and GRACE datasets.

The compression ratio — defined as raw forcing volume divided by basin-averaged forcing volume — exhibits a crossover that depends on the ratio of target HRUs to source grid cells. At the point scale (Paradise, 1 HRU vs 9 grid cells), the ratio is 0.12, indicating substantial spatial aggregation. At the watershed scale (Bow, 49 HRUs vs 42 grid cells), the ratio is 0.73 — near 1:1, as HRU count approaches the source grid count. At the regional scale (Iceland, 21,474 HRUs vs 954 grid cells), the ratio drops to 0.06: basin-averaged forcing (3.4 GB) is 18× larger than raw forcing (191 MB), reflecting the 22:1 expansion from grid cells to HRUs. This crossover represents a fundamental trade-off in spatial discretisation; for operational deployments where storage and I/O bandwidth are constrained, this analysis quantifies the cost of spatial resolution.

## 4.12.2 Forcing Data Transformation

To illustrate how the pipeline transforms raw reanalysis data into model-ready forcing, we trace ERA5 temperature through the Bow at Banff domain (Figure Y).

### Spatial remapping

The ERA5 bounding box for the Bow domain comprises a 6 × 7 grid of 42 cells at 0.25° resolution, spanning 50.75–52.0°N and 116.75–115.25°W. Figure Ya shows this grid overlaid on the 49 semi-distributed HRU polygons, illustrating the spatial scale mismatch between the forcing grid and the hydrological discretisation. The EASYMORE intersection algorithm identifies 102 grid-cell × HRU overlaps — each of the 49 HRUs intersects on average 2.1 ERA5 cells — and computes area-proportional weights normalised to sum to 1.0 for each HRU.

Figure Yb displays the raw ERA5 temperature field at a representative time step (mid-month snapshot), showing the coarse 6 × 7 grid with within-basin temperature gradients of approximately 6 K driven by the 1,000 m elevation range. The white HRU outlines demonstrate the spatial resolution gap that remapping must bridge. Figure Yc shows the result after weight application: HRU-level temperatures are plotted as a choropleth with the raw ERA5 field interpolated as contours beneath, confirming that the area-weighted remapping preserves the large-scale gradient while redistributing values to the hydrological discretisation. The axes are zoomed to the basin extent, revealing how high-elevation HRUs in the northwest receive colder temperatures than low-elevation HRUs near the outlet.

### Variable transformation and lapse-rate correction

Beyond spatial remapping, the pipeline applies two additional transformations to each forcing variable. First, variable standardisation converts ERA5-native representations to SYMFLUENCE's CF-Intermediate Format (CFIF): accumulated energy fluxes (J m⁻²) become instantaneous fluxes (W m⁻²), accumulated precipitation (m) becomes precipitation rate (kg m⁻² s⁻¹), dew-point temperature is converted to specific humidity, and U/V wind components are combined into scalar wind speed. Two variables (temperature, surface pressure) pass through with unit preservation. This standardisation layer decouples dataset-specific conventions from model-specific input requirements; the current implementation supports 10+ forcing datasets and 10 hydrological models through approximately 20 adapter implementations rather than the ~100 that would be required without the intermediate format.

**Table 1.** ERA5 to CFIF variable mapping applied identically across all three domains.

| CFIF variable | ERA5 source | ERA5 units | CFIF units | Transformation |
|---------------|-------------|------------|------------|----------------|
| `airtemp` | `t2m` | K | K | Direct pass-through |
| `pptrate` | `tp` | m (accumulated) | kg m⁻² s⁻¹ | De-accumulation, unit conversion |
| `LWRadAtm` | `strd` | J m⁻² (accumulated) | W m⁻² | De-accumulation, J→W conversion |
| `SWRadAtm` | `ssrd` | J m⁻² (accumulated) | W m⁻² | De-accumulation, J→W conversion |
| `spechum` | `d2m`, `t2m` | K (dew point) | kg kg⁻¹ | Derived from dew-point temperature |
| `windspd` | `u10`, `v10` | m s⁻¹ (components) | m s⁻¹ | Vector magnitude from U, V |
| `airpres` | `sp` | Pa | Pa | Direct pass-through |

Second, elevation lapse-rate correction adjusts temperature for the difference between the ERA5 grid cell elevation and each HRU's mean elevation. The Bow domain spans 1,472–2,454 m across its 49 HRUs, while ERA5 grid cell elevations are smoothed to the 0.25° topography. Figure Yd shows the basin-averaged temperature time series at three processing stages — raw spatial mean, HRU-weighted basin average, and SUMMA-ready (after lapse correction) — over 14 days. The mean absolute difference between the raw spatial mean and the basin-weighted average is 2.0 K, reflecting the effect of area-weighted remapping. The lapse-corrected (SUMMA-ready) signal diverges further, particularly at high-elevation HRUs.

Figure Ye presents the lapse-rate relationship explicitly: temperature at each HRU plotted against elevation, before and after correction, alongside the theoretical −6.5 K km⁻¹ environmental lapse rate. Before correction, all HRUs at a given time step share similar temperatures (reflecting the coarse ERA5 grid); after correction, the temperature–elevation relationship aligns with the theoretical gradient. Figure Yf maps the spatial pattern of the lapse-rate correction (ΔT = corrected − uncorrected) as a diverging choropleth. High-elevation HRUs in the northwest receive negative corrections (colder), while low-elevation HRUs near the outlet receive positive corrections (warmer), with the magnitude reaching ±3 K for the most extreme elevation differences.

## 4.12.3 Observation Processing

Observation data enters the pipeline through two parallel processing paths (Figure Za): station-based point measurements and remote sensing gridded products. Both paths share a common structure — configuration, acquisition, quality control, and temporal alignment — but differ in their spatial processing requirements.

### Station data

Station observations require no spatial processing: the pipeline downloads time series from provider APIs (Water Survey of Canada for streamflow, SNOTEL for snow), applies quality-control filters, fills short gaps where configured, and aligns the temporal resolution to the experiment period. Figure Zb illustrates the pipeline output for WSC station 05BB001 (Bow at Banff): 394,441 hourly discharge records spanning 1979–2023 (45 years) with 0.0% gap fraction, a peak discharge of 466 m³ s⁻¹, and a mean of 37.6 m³ s⁻¹. The processed time series exhibits the characteristic nival regime of the Canadian Rockies — low winter baseflow (5–15 m³ s⁻¹) punctuated by pronounced snowmelt freshets peaking in June–July. The calibration (2004–2007) and evaluation (2008–2009) experiment periods used in Sections 4.4–4.7 are highlighted, demonstrating that the pipeline produces records far exceeding the experiment window, enabling flexible period selection without re-processing.

### Remote sensing data

Remote sensing products require an additional spatial extraction step: the pipeline downloads gridded fields (GRACE Mascon solutions for total water storage, MODIS for ET and snow cover area), clips or area-averages them to the catchment boundary using the domain shapefile, and produces basin-integrated time series. Figure Zc shows the processed GRACE total water storage (TWS) anomaly for the Iceland domain (Ellidaar catchment): 283 monthly records from April 2002 to October 2025, with 35 temporal gaps (12.4% missing, primarily during the 11-month gap between GRACE and GRACE-FO in 2017–2018). Three independent Mascon solutions (JPL, CSR, GSFC) are processed in parallel through the same pipeline, providing an observational uncertainty envelope. The inter-solution spread is largest during periods of rapid mass change, reaching ±20 mm w.e. during spring melt, while the three solutions converge during winter accumulation periods.

### Observation coverage across scales

Observation processing varies substantially across domains, reflecting both data availability and modelling objectives. Paradise (Sections 4.3, 4.10) has 2 observation types: SNOTEL snow (7,996 records, 2001–2023, 0% gaps) and soil moisture (5,005 records, 2011–2025, 0.6% gaps), totalling 3.5 MB. Bow (Sections 4.2, 4.4–4.7) has 2 types: streamflow (385,681 records, 1980–2023, 0% gaps) totalling 14.1 MB. Iceland (Sections 4.8, 4.9) has 4 types: streamflow (458,761 records, 1969–2021, 8.4% gaps), GRACE TWS (283 records, 2002–2025, 12.4% gaps), and ET, totalling 194 MB. The observation volume for Iceland is dominated by GRACE (171 MB across three Mascon NetCDF files plus the processed CSV), illustrating how remote sensing products can exceed station data in storage requirements despite having far fewer temporal records.

## 4.12.4 Framework Implications

Four aspects of these cross-scale results bear on SYMFLUENCE's design objectives.

First, the pipeline's **declarative control** — where switching between a point-scale SNOTEL experiment and a 21,474-HRU regional application requires changing only the domain coordinates, discretisation parameters, and observation sources in the YAML configuration — demonstrates that data preprocessing complexity is absorbed by the framework rather than imposed on the user. The same 16-stage DAG with 25 data-flow edges executes identically across all three scales, with the framework automatically adapting matrix dimensions, data acquisition extents, and observation source selection.

Second, the **weight caching architecture** becomes increasingly important at larger scales. For the Bow domain, EASYMORE weights (1.1 MB, 102 intersections) are computed once and reused across dozens of calibration trials, model configurations, and evaluation experiments (Sections 4.2–4.7). For Iceland, the weights are substantially larger (221 MB, 29,933 intersections) but the amortisation benefit is proportionally greater: a single weight computation serves all large-domain and large-sample experiments (Sections 4.8–4.9).

Third, the **CF-Intermediate Format** provides a well-defined interface between the data acquisition layer and the model execution layer that is invariant to scale. The same ERA5 → CFIF mapping (7 variables, 5 transformation types; Table 1) produces identically structured outputs regardless of whether the target is 1 HRU or 21,474 HRUs. This scale-invariant interface enables independent evolution of both layers: new forcing datasets can be added without modifying any model code, and new models can be integrated without modifying any data processing code.

Fourth, the **scaling analysis** provides quantitative guidance for operational deployments. The compression crossover — where basin-averaged forcing exceeds raw forcing in volume — occurs when the HRU count approaches the ERA5 grid cell count intersecting the domain. Below this threshold (Paradise: 1 HRU vs 9 cells), spatial aggregation reduces storage; above it (Iceland: 21,474 HRUs vs 954 cells), storage increases proportionally with HRU count. This relationship, combined with the one-time weight computation cost, allows practitioners to estimate pipeline resource requirements before committing to a particular spatial discretisation strategy. For the three domains examined here, total pipeline output spans three orders of magnitude — from 12.3 MB (point) to 4.3 GB (regional) — providing concrete benchmarks for planning storage, compute, and I/O requirements at intermediate scales.
