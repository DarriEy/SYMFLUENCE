## 4.5 Benchmarking

To contextualise the multi-model ensemble skill reported in Section 4.2, we evaluate the six calibrated models against a hierarchy of simple reference predictors following the benchmarking philosophy of Schaefli and Gupta (2007). A model whose KGE barely exceeds that of the long-term mean flow offers limited scientific value, whereas the same KGE is meaningful if no simple predictor comes close. SYMFLUENCE's `Benchmarker` class wraps the HydroBM library (Knoben, 2024) and computes 19 benchmark flow series from observed streamflow and ERA5 precipitation for the Bow River at Banff domain used in Section 4.2 (lumped, 2,210 km²; 2004–2009). After removing five benchmarks that produce invalid validation metrics and consolidating seven that yield numerically identical flow series on this domain, 12 distinct benchmarks remain, spanning four categories: time-invariant (mean and median flow), seasonal (monthly and daily climatologies), rainfall-runoff ratio (at annual through daily aggregation), and the Schaefli and Gupta (2007) precipitation-based benchmarks.

### 4.5.1 Benchmark performance hierarchy

Figure X presents the benchmarking results. Panel (a) compares the validation-period KGE of the 12 deduplicated benchmarks against the six calibrated models and the ensemble mean and median from Section 4.2. Panel (b) shows calibration and validation KGE for each benchmark grouped by category, illustrating both absolute skill and transferability.

The benchmarks exhibit a steep performance hierarchy. Time-invariant predictors — the long-term mean and median flow — perform poorly (KGE = −0.41 and −0.47, respectively), confirming that a constant-discharge predictor has no meaningful skill for this nival regime. Seasonal climatologies, by contrast, capture the dominant snowmelt cycle and achieve substantially higher skill: the daily median flow benchmark attains the highest validation KGE of any reference predictor (0.80), followed closely by the daily mean (0.80), monthly mean (0.76), and monthly median (0.75). The narrow spread within the seasonal group indicates that the dominant source of predictability at Bow-at-Banff is the annual snowmelt cycle, and that resolving day-of-year climatology adds only a modest increment over monthly climatology.

Rainfall-runoff benchmarks show mixed performance. When precipitation is aggregated to monthly totals before computing the runoff ratio, the resulting benchmark achieves a validation KGE of 0.76 — comparable to the seasonal climatologies — because the monthly aggregation effectively filters sub-monthly noise. At daily resolution, however, the rainfall-runoff ratio benchmark degrades sharply (KGE = 0.23), reflecting the poor correspondence between daily ERA5 precipitation and same-day streamflow in a snowmelt-dominated catchment where storage processes decouple precipitation timing from runoff generation. The Schaefli and Gupta (2007) precipitation-based benchmarks follow a similar pattern: the adjusted smoothed variant (which applies temporal smoothing) reaches KGE = 0.73, while the unsmoothed scaled precipitation benchmark achieves only 0.23.

Panel (b) reveals that calibration-to-validation degradation varies by benchmark category. Seasonal benchmarks are remarkably stable (calibration KGE 0.82–0.91, validation 0.75–0.80), consistent with the stationarity of the seasonal cycle over the six-year record. Rainfall-runoff benchmarks are less stable, with the monthly variant improving from calibration (0.61) to validation (0.76) and the daily variant improving substantially from −0.31 to 0.23, suggesting that the HydroBM calibration-period split (2004–2006 vs. 2007–2009) captures different rainfall-runoff dynamics than the model ensemble's split (2004–2007 vs. 2008–2009).

### 4.5.2 Model ensemble vs. benchmarks

Returning to panel (a), five of the six calibrated models exceed all 12 valid benchmarks. LSTM (KGE = 0.88), SUMMA (0.88), and FUSE (0.88) lead by a margin of +0.08 above the best benchmark; GR4J (0.79) and HYPE (0.81) also surpass the daily median benchmark, though by narrower margins (+0.05 and +0.01, respectively). HBV is the sole model that falls below the best benchmark (KGE = 0.70 vs. 0.80), consistent with the systematic negative bias identified in its KGE decomposition (Section 4.2.2). Its evaluation KGE still exceeds all time-invariant and most rainfall-runoff benchmarks, indicating that the model captures genuine hydrological signal, but its performance is comparable to what a simple seasonal climatology achieves.

The ensemble mean hydrograph (KGE = 0.94) exceeds the best benchmark by +0.14, and the ensemble median (0.92) by +0.12. These margins confirm that the multi-model ensemble provides substantial predictive value beyond reference models: the gap between the best benchmark (0.80) and the ensemble mean (0.94) is roughly twice the gap between the best and worst seasonal benchmarks (0.80 vs. 0.75), indicating that the calibrated models contribute information well beyond what the seasonal cycle alone provides.

### 4.5.3 Framework implications

Figure Y illustrates the information content of the benchmark hierarchy through flow time series. Panel (a) shows the evaluation period (2008–2009), overlaying the observed hydrograph with the best seasonal benchmark (daily median) and the seasonal benchmark envelope. The daily median tracks the observed seasonal pattern — winter baseflow and the June freshet — but cannot capture inter-annual variability in peak magnitude or the timing of individual storm events. Panel (b) displays the full record (2004–2009) with benchmarks of increasing temporal resolution: the constant mean flow, monthly and daily climatologies, and the monthly rainfall-runoff ratio. The stepwise addition of temporal information is clearly visible, from a flat line to monthly steps to daily oscillations, yet even the most resolved benchmark fails to reproduce the year-to-year differences in freshet timing and magnitude that the calibrated models capture.

The benchmarking analysis demonstrates two aspects of SYMFLUENCE's evaluation framework. First, the `Benchmarker` integrates seamlessly with the configuration-driven workflow: benchmarks are computed from the same preprocessed forcing and observation data used for model calibration, and the results are written alongside model evaluation metrics in a standardised format. This ensures that benchmark comparisons are reproducible and internally consistent without requiring manual data handling. Second, by quantifying the performance floor established by simple predictors, benchmarking guards against over-interpreting modest KGE values. The daily median benchmark's KGE of 0.80 sets a high bar for this catchment — a model achieving KGE = 0.82 contributes only marginal information beyond the seasonal cycle, while the ensemble mean's KGE of 0.94 represents a clear and substantial advance. Integrating benchmarking as a standard evaluation step encourages users to report model skill relative to reference predictors, strengthening the interpretability of performance claims across studies.
