% From Configuration to Prediction - WRR Submission
% AGU Journal Template for Water Resources Research
\documentclass[draft,wrcr]{agujournal2019}

% Required packages
\usepackage{apacite}
\usepackage{lineno}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{url}
\usepackage{multirow}

% Line numbers for review
\linenumbers

\begin{document}

%% Title
\title{From Configuration to Prediction: Multi-Model, Multi-Basin Experiments with SYMFLUENCE}

%% Authors
\authors{
Darri Eythorsson\affil{1},
Nicolas Vasquez\affil{1},
Cyril Th\'{e}bault\affil{1},
Frank Han\affil{1},
Kasra Keshavarz\affil{1},
Wouter Knoben\affil{1},
Dave Casson\affil{1},
Mohammed Ismail Ahmed\affil{1},
Ashley Van Beusekom\affil{1},
Hongli Liu\affil{2},
Befekadu Taddesse Woldegiorgis\affil{1},
Camille Gautier\affil{1},
Katherine Reece\affil{1},
Peter Wagener\affil{1},
Ignacio Aguirre\affil{1},
Paul Coderre\affil{1},
Neharika Bhattarai\affil{1},
Junwei Guo\affil{1},
Shadi Hatami\affil{1},
David Tarboton\affil{5},
James Halgren\affil{4},
Jordan Reed\affil{7},
Steve Burian\affil{4},
Raymond Spiteri\affil{3},
Alain Pietroniro\affil{1},
Martyn Clark\affil{1}
}

%% Affiliations
\affiliation{1}{University of Calgary, Schulich School of Engineering, Department of Civil Engineering, Calgary, Alberta, Canada}
\affiliation{2}{University of Alberta, Faculty of Engineering, Department of Civil and Environmental Engineering, Edmonton, Alberta, Canada}
\affiliation{3}{University of Saskatchewan, Department of Computer Science, Saskatoon, Saskatchewan, Canada}
\affiliation{4}{Brigham Young University, Department of Civil Engineering, Provo, Utah, USA}
\affiliation{5}{Utah State University, Department of Civil Engineering, Logan, Utah, USA}
\affiliation{7}{Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI), USA}

%% Corresponding author
\correspondingauthor{Darri Eythorsson}{darri.eythorsson@ucalgary.ca}

%% Key Points
\begin{keypoints}
\item A six-model ensemble achieves KGE = 0.94, exceeding every individual model through cancellation of structural biases
\item Intra-model structural uncertainty (64 FUSE configurations, KGE range 2.74) rivals inter-model uncertainty
\item Scale-invariant workflows enable experiments from point-scale flux towers to 21,474-HRU regional simulations
\end{keypoints}

%% ============================================================================
%% ABSTRACT
%% ============================================================================
\begin{abstract}
The companion papers in this series argued that predictive stagnation in computational hydrology reflects an infrastructure deficit rather than a scientific one, and described the four-tier layered architecture, registry-based extensibility, and declarative configuration system through which SYMFLUENCE addresses that deficit. This paper subjects that architecture to empirical test. Through twelve experiments spanning point-scale flux estimation to regional hydrological simulation across Iceland ($\sim$103,000~km$^2$), we demonstrate that the architectural principles described in the companion papers translate into practical experimental capability. We present: (i) domain definition experiments demonstrating scale-invariant workflow execution from a single flux tower to 21,474 hydrological response units; (ii) a six-model ensemble comparing structurally diverse models (SUMMA, FUSE, GR4J, HBV, HYPE, LSTM) on a common testbed, where the ensemble mean (KGE = 0.94) exceeds every individual model; (iii) a forcing ensemble isolating the effect of seven meteorological products on snow simulation, revealing forcing-dependent parameter compensation that undermines transferability; (iv) a twelve-algorithm calibration comparison exposing bimodal parameter regimes and demonstrating that gradient-based methods achieve superior generalization; (v) a 64-member FUSE structural decision ensemble showing that intra-model structural uncertainty rivals inter-model uncertainty; (vi) cross-model sensitivity analysis identifying soil storage and routing as consistently influential processes; (vii) a 111-catchment large-sample application across Iceland; (viii) a fully distributed regional simulation with explicit river routing; (ix) multivariate evaluation against GRACE, MODIS, and SMAP satellite observations; (x) parallel scaling characterization; and (xi) end-to-end data processing pipeline analysis across three orders of magnitude in domain size. The experiments demonstrate that when technical friction is reduced through deliberate architectural design, experiments that would otherwise be prohibitively labor-intensive become routine.
\end{abstract}

%% ============================================================================
%% PLAIN LANGUAGE SUMMARY
%% ============================================================================
\section*{Plain Language Summary}
Our companion papers argued that hydrological prediction has stalled not because of scientific limits but because our software tools are fragmented, and described a system called SYMFLUENCE designed to fix this. This paper tests whether that system actually works by running twelve different experiments. We compared six completely different hydrological models on the same river basin and found that averaging their predictions (KGE = 0.94) beat every individual model---the models' errors partially cancel out. We tested how different weather datasets affect snow predictions and discovered that models compensate for bad weather data by learning unrealistic parameter values that don't transfer to new conditions. We compared twelve calibration algorithms and found that newer gradient-based methods generalize better than traditional approaches. We explored 64 different structural configurations within a single model framework (FUSE) and found the spread in performance was as large as the spread between completely different models---structural choices within a model matter as much as model choice itself. We ran the same workflow on domains ranging from a single weather station to all of Iceland (21,474 computational units), demonstrating that the system scales without code changes. These experiments would have taken months of manual work using traditional approaches. With SYMFLUENCE, each required only a configuration file change, showing that better infrastructure expands the questions researchers can ask.

%% ============================================================================
%% MAIN TEXT
%% ============================================================================

\section{Introduction}

The first paper in this series \cite{Eythorsson2025a} documented the impossible generalist problem and argued that the apparent stagnation in hydrological prediction reflects infrastructure fragmentation rather than scientific limits. The second paper \cite{Eythorsson2025b} described the architectural response: SYMFLUENCE's four-tier layered design, registry-based extensibility, declarative configuration system, CF-Intermediate Format, and scale-invariant workflow orchestration. This third paper asks the question that architecture alone cannot answer: \textit{does it work?}

The experiments presented here are designed to test the architectural claims made in the companion papers against concrete hydrological problems. Each experiment isolates a specific source of uncertainty---model structure, forcing data, calibration algorithm, intra-model structural decisions, spatial discretization---while holding all other factors constant through SYMFLUENCE's configuration-driven design.

We organize the experiments along two axes. The first axis is \textit{spatial scale}: from point-scale flux tower validation (Paradise SNOTEL), through watershed-scale process modeling (Bow River at Banff, 2,210~km$^2$), to regional-scale distributed simulation (Iceland, $\sim$103,000~km$^2$). The second axis is \textit{experimental design}: single-model calibration, multi-model ensemble, forcing ensemble, algorithm comparison, structural decision analysis, sensitivity screening, large-sample application, multivariate evaluation, and computational scaling.

\subsection{Experimental Overview}

Table~\ref{tab:experiments} summarizes the twelve experiments, the architectural features they exercise, and the sections in which they are described.

\begin{table}[ht]
\centering
\caption{Overview of experiments presented in this paper.}
\label{tab:experiments}
\begin{tabular}{clllll}
\toprule
\# & Experiment & Section & Domain & Models & Key Feature Tested \\
\midrule
1 & Domain definition & 2 & Paradise, Bow, Iceland & -- & Scale-invariant discretization \\
2 & Multi-model ensemble & 3 & Bow at Banff & 6 & Registry-based integration \\
3 & Forcing ensemble & 4 & Paradise & SUMMA & Configuration-driven isolation \\
4 & Algorithm comparison & 5 & Bow at Banff & HBV & Algorithm registry \\
5 & Benchmarking & 6 & Bow at Banff & 6 & Evaluation framework \\
6 & Decision ensemble & 7 & Bow at Banff & FUSE (64) & Structural analysis \\
7 & Sensitivity analysis & 8 & Bow at Banff & 5 & Cross-model mapping \\
8 & Large-sample & 9 & Iceland (111) & FUSE & Batch scaling \\
9 & Large-domain & 10 & Iceland (full) & FUSE+mizuRoute & Distributed routing \\
10 & Multivariate evaluation & 11 & Multiple & SUMMA & Satellite handlers \\
11 & Parallel scaling & 12 & Bow at Banff & HBV, SUMMA & Execution strategies \\
12 & Data pipeline & 13 & Multiple & -- & CFIF, weight caching \\
\bottomrule
\end{tabular}
\end{table}

%% ============================================================================
\section{Domain Definition Across Scales}

SYMFLUENCE supports three fundamental spatial modes---point, watershed, and regional---enabling consistent workflows from single-site validation to continental-scale simulation. We demonstrate this flexibility through applications at Paradise SNOTEL (Washington, USA), the Bow River at Banff (Alberta, Canada), and the national domain of Iceland.

\subsection{Point-Scale: Paradise SNOTEL}

Point-scale applications treat the domain as a single computational unit without lateral routing. The Paradise SNOTEL station (46.78$^\circ$N, 121.75$^\circ$W; elevation 1,560~m) on Mount Rainier receives approximately 2,500~mm annual precipitation, predominantly as snow. Configuration requires only pour point coordinates and a bounding box---approximately 15 lines of YAML.

\subsection{Watershed-Scale: Bow River at Banff}

The Bow River basin upstream of Banff (51.17$^\circ$N, 115.57$^\circ$W; approximately 2,210~km$^2$, 20 ERA5 cells) spans elevations from 1,383~m to over 3,436~m. SYMFLUENCE supports lumped mode (single GRU), semi-distributed mode (49 sub-basin GRUs), and distributed mode (2,335 grid cells). Hybrid configurations---lumped GRUs with semi-distributed routing---capture elevation-dependent processes through lapse-rate corrections while routing runoff through a spatially explicit river network.

\subsection{Regional-Scale: Iceland}

Regional configuration specifies a bounding box rather than a pour point, with automatic delineation of all watersheds draining to the coast. The resulting domain comprises 6,600 GRUs connected by 6,606 river segments, with elevation-band subdivision yielding 21,474 HRUs.

Figure~\ref{fig:domains} illustrates the three spatial scales, demonstrating that the same workflow engine produces consistent domain definitions from point to regional scale.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/fig_domain_definition.pdf}
\caption{Domain definition across scales. (a) Paradise SNOTEL point-scale domain with ERA5 forcing grid overlay. (b) Bow River at Banff watershed with lumped, semi-distributed, and distributed discretizations. (c) Iceland regional domain showing the 6,600 GRU mesh and river network.}
\label{fig:domains}
\end{figure}

%% ============================================================================
\section{Multi-Model Ensemble}

To demonstrate SYMFLUENCE's capacity for orchestrating structurally diverse models within a unified workflow, we configured an ensemble of six hydrological models for the Bow River at Banff: SUMMA \cite{Clark2015a, Clark2015b}, FUSE \cite{Clark2008}, GR4J \cite{Perrin2003}, HBV, HYPE, and LSTM. Each model was calibrated against observed daily streamflow using DDS \cite{TolsonShoemaker2007} over 2003--2005, with independent evaluation over 2006--2009.

\begin{table}[ht]
\centering
\caption{Multi-model ensemble members and performance.}
\label{tab:ensemble}
\begin{tabular}{llrrr}
\toprule
Model & Paradigm & $n$ & Cal.\ KGE & Eval.\ KGE \\
\midrule
SUMMA & Process-based & 11 & 0.90 & 0.88 \\
FUSE & Flexible conceptual & 13 & 0.90 & 0.88 \\
GR4J & Parsimonious conceptual & 4 & 0.92 & 0.79 \\
HBV & Conceptual & 15 & 0.74 & 0.70 \\
HYPE & Conceptual & 10 & 0.87 & 0.81 \\
LSTM & Machine learning & -- & 0.97 & 0.88 \\
\midrule
Ensemble mean & -- & -- & -- & 0.94 \\
\bottomrule
\end{tabular}
\end{table}

The ensemble mean (KGE = 0.94) exceeds every individual model \cite{Gupta2009, Kling2012}. This improvement arises from the cancellation of offsetting structural biases \cite{Ajami2006, Arsenault2015}: HBV's negative bias and GR4J's positive bias partially neutralize in the average, while correlation is preserved across all members.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/fig3_multimodel_hydrograph.pdf}
\caption{Simulated and observed hydrographs over the full analysis period. (a) All six models with calibration--evaluation boundary at January 2006. (b) Zoom into the April--October 2005 snowmelt season highlighting differences in peak timing and magnitude.}
\label{fig:hydrograph}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/fig4_kge_decomposition.pdf}
\caption{KGE decomposition into correlation ($r$), variability ratio ($\alpha$), and bias ratio ($\beta$) for calibration and evaluation periods. HBV shows persistent negative bias ($\beta = 0.75$), while all models maintain high correlation ($r > 0.87$).}
\label{fig:kge_decomp}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/fig5_ensemble_fdc.pdf}
\caption{Ensemble evaluation. (a) Ensemble envelope (min--max range) for the evaluation period with ensemble mean (KGE = 0.94) and median (KGE = 0.92). (b) Flow duration curves showing that the ensemble mean tracks the observed low-flow regime more faithfully than any single model.}
\label{fig:ensemble_fdc}
\end{figure}

%% ============================================================================
\section{Forcing Ensemble}

Holding model structure fixed (SUMMA at Paradise SNOTEL), we calibrated an identical set of 11 snow-related parameters against observed daily SWE under each of 14 forcing datasets. The calibration period spans water years 2016--2018, with independent evaluation over water years 2019--2020. The 14 datasets comprise four observation-based reanalysis products---ERA5 ($\sim$31~km), AORC ($\sim$1~km), CONUS404 ($\sim$4~km), and RDRS ($\sim$10~km)---and ten members of the NEX-GDDP-CMIP6 downscaled climate projections ($\sim$25~km): ACCESS-CM2, CanESM5, CNRM-CM6-1, GFDL-ESM4, INM-CM5-0, IPSL-CM6A-LR, MPI-ESM1-2-HR, MRI-ESM2-0, NorESM2-LM, and UKESM1-0-LL.

Figure~\ref{fig:forcing} presents the forcing ensemble results. Key findings: (1) Calibration performance is not a reliable predictor of evaluation performance---RDRS achieves the highest calibration KGE (0.94) but degrades by 0.35, while AORC actually \textit{improves} during evaluation (Cal KGE = 0.77, Eval KGE = 0.87). (2) ERA5 collapses to evaluation KGE = $-$0.59, reflecting persistent precipitation biases at this maritime mountain site. (3) The \texttt{frozenPrecipMultip} parameter spans a five-fold range across forcings (0.84 for CONUS404 to 4.98 for MRI-ESM2-0), indicating forcing-dependent parameter compensation that undermines transferability. (4) The ten GDDP members show evaluation KGE ranging from 0.08 (CNRM-CM6-1) to 0.81 (NorESM2-LM), reflecting the heterogeneous quality of the underlying GCM precipitation fields.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/fig_forcing_ensemble.pdf}
\caption{Forcing ensemble results. (a) SWE time series for each forcing dataset during calibration and evaluation periods. (b) Performance metrics (KGE components) by forcing dataset. (c) Parameter divergence across forcings, showing the five-fold range in \texttt{frozenPrecipMultip}.}
\label{fig:forcing}
\end{figure}

%% ============================================================================
\section{Calibration Algorithm Comparison}

We compare 12 calibration algorithms on the HBV model at Bow River: DDS \cite{TolsonShoemaker2007}, Nelder-Mead, GA, CMA-ES, ADAM, SA, DE, SCE-UA \cite{Duan1992}, Bayesian Optimization, PSO, Basin Hopping, and L-BFGS. All algorithms calibrate the same 14 parameters against streamflow using KGE \cite{Gupta2009}, with approximately 4,000 function evaluations \cite{Arsenault2014}.

Figure~\ref{fig:calibration} presents the algorithm comparison results. Key findings: (1) The calibration KGE range is narrow (0.654--0.757), indicating the objective surface is relatively accessible. (2) Six algorithms perform better in evaluation than calibration---ADAM achieves the highest evaluation KGE (0.778) despite ranking fifth in calibration, suggesting the smoothed HBV formulation required by gradient methods acts as an implicit regularizer. (3) Algorithms reveal bimodal parameter regimes: a ``warm threshold / fast melt'' regime versus a ``cold threshold / slow melt'' regime (Figure~\ref{fig:calibration}c).

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/fig_calibration_comparison.pdf}
\caption{Calibration algorithm comparison. (a) Calibration and evaluation KGE by algorithm. (b) Convergence trajectories showing function evaluations versus objective value. (c) Parameter space projections revealing bimodal regimes.}
\label{fig:calibration}
\end{figure}

%% ============================================================================
\section{Benchmarking}

We evaluate the six calibrated models against 12 reference predictors \cite{SchaefliGupta2007, Knoben2024}: time-invariant (mean, median flow), seasonal climatologies (monthly mean, daily median), and rainfall-runoff ratios.

Figure~\ref{fig:benchmark} presents the benchmarking results. The daily median flow benchmark achieves KGE = 0.80---five of six calibrated models exceed all benchmarks, with the ensemble mean exceeding the best benchmark by +0.14. HBV is the sole model that falls below the best benchmark (KGE = 0.70 vs.\ 0.80).

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/fig_benchmarking.pdf}
\caption{Benchmarking results. Performance of six calibrated models compared against 12 reference predictors. The ensemble mean exceeds the best benchmark (daily median flow) by +0.14 KGE units.}
\label{fig:benchmark}
\end{figure}

%% ============================================================================
\section{Model Decision Ensemble}

Using FUSE's configurable structure \cite{Clark2008, Clark2011}, we select six decision dimensions with two options each for a full-factorial design of $2^6 = 64$ combinations: upper-layer architecture, lower-layer architecture, surface runoff generation, percolation, evaporation, and interflow.

\begin{table}[ht]
\centering
\caption{FUSE structural decisions varied in the ensemble.}
\label{tab:fuse}
\begin{tabular}{llll}
\toprule
Decision & Description & Option A & Option B \\
\midrule
ARCH1 & Upper-layer soil & tension1\_1 & onestate\_1 \\
ARCH2 & Lower-layer soil & tens2pll\_2 & unlimfrc\_2 \\
QSURF & Surface runoff & arno\_x\_vic & prms\_varnt \\
QPERC & Percolation & perc\_f2sat & perc\_lower \\
ESOIL & Evaporation & sequential & rootweight \\
QINTF & Interflow & intflwnone & intflwsome \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:fuse_ensemble} presents the structural decision ensemble results. Key findings: (1) Calibrated KGE ranges from $-$1.89 to 0.86 (spread of 2.74)---structural uncertainty \textit{within} FUSE is of the same order as uncertainty \textit{between} models. (2) Main effects explain 34.1\% of variance, two-way interactions 35.7\%, and residual 30.2\%---interactions dominate. (3) The Percolation $\times$ Interflow interaction alone accounts for 19.1\% of variance. (4) All nine catastrophic structures (KGE $<$ 0) share the combination \texttt{perc\_f2sat} + \texttt{intflwsome}.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/fig_fuse_ensemble.pdf}
\caption{FUSE structural decision ensemble. (a) Performance distribution across 64 configurations showing KGE spread of 2.74. (b) Main effects and interactions. (c) Variance decomposition showing that two-way interactions (35.7\%) exceed main effects (34.1\%). (d) Identification of catastrophic structural combinations.}
\label{fig:fuse_ensemble}
\end{figure}

%% ============================================================================
\section{Sensitivity Analysis}

Using calibration trajectories from five models (FUSE, GR4J, HBV, HYPE, SUMMA), we computed sensitivity indices via Spearman rank correlation and RBD-FAST. Parameters were mapped to eight hydrological processes.

Figure~\ref{fig:sensitivity} presents the cross-model sensitivity results. Key findings: \textbf{Soil Storage} ranks among the top three most sensitive processes in all four models that represent it. \textbf{Routing} is similarly prominent. Snow parameters show moderate but consistent sensitivity (0.54--0.74). ET parameters are weakly sensitive, suggesting they are less constrained by streamflow-only calibration.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/fig_sensitivity.pdf}
\caption{Cross-model sensitivity analysis. Process-level sensitivity indices computed via RBD-FAST for five models, with parameters mapped to eight hydrological processes. Soil storage and routing consistently rank among the most sensitive processes.}
\label{fig:sensitivity}
\end{figure}

%% ============================================================================
\section{Large-Sample Application}

The preceding experiments (Sections 2--8) demonstrate SYMFLUENCE's modelling capabilities on individual catchments. A framework intended for operational and research use must, however, scale to many catchments with minimal per-basin manual intervention. To evaluate this, we apply a single FUSE configuration template across 111 LamaH-Ice catchments \cite{HelgasonNijssen2024} spanning the full hydrological diversity of Iceland (Figure~\ref{fig:large_sample}).

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/fig_large_sample.pdf}
\caption{Overview of the 111 LamaH-Ice study catchments. (a) Catchment boundaries coloured by drainage area (km$^2$), overlaid on the national GRU mesh; dashed red outlines indicate catchments with glacier fraction exceeding 30\%. (b) Distribution of catchment area (log scale). (c) Distribution of streamflow record length.}
\label{fig:large_sample}
\end{figure}

\subsection{Study Domain and Experimental Design}

Across the 111 catchments, drainage areas span three orders of magnitude (3.8--7,437~km$^2$, median 391~km$^2$), mean elevations range from 39 to 1,307~m, and 63 catchments (57\%) contain glacierized area---with glacier fractions reaching 93\% in the most heavily glacierized basin. Streamflow records range from 4 to 89 years (median 33 years), reflecting the heterogeneous monitoring history of Iceland's hydrometric network.

All 111 catchments are configured for lumped FUSE modelling with ERA5 reanalysis forcing at hourly resolution (0.25$^\circ$ grid). DDS \cite{TolsonShoemaker2007} is used for calibration with 1,000 iterations and KGE as the objective function. Thirteen FUSE parameters are calibrated with identical parameter bounds across all catchments. Where streamflow records cover the 2005--2014 window, a standardised period split is applied: two-year spinup (2005--2006), five-year calibration (2007--2011), and three-year evaluation (2012--2014); 63 catchments satisfy this criterion. The remaining 48 catchments are assigned catchment-specific periods determined automatically from the available observation window, maintaining the same proportional split.

\subsection{Configuration and Execution}

The 111 FUSE configurations are generated programmatically from a single YAML template. A setup script reads each catchment's streamflow record to determine the observation window, computes appropriate time-period splits, derives bounding-box coordinates from the catchment shapefile, and writes the per-domain configuration file. The only fields that vary between configurations are the domain identifier, bounding-box coordinates, and the five time-period settings.

Execution proceeds sequentially via a runner script that invokes the SYMFLUENCE workflow for each domain, skips previously completed catchments, and logs per-domain return codes and elapsed times to a persistent log file. This design allows the campaign to be interrupted and resumed without re-running completed domains.

\subsection{Framework Implications}

This experiment exercises three aspects of SYMFLUENCE's architecture that single-catchment studies do not test. First, the configuration-driven design enables systematic scaling from one catchment to many without code modification. Second, the standardised directory structure and output conventions ensure that results from all catchments are directly comparable and machine-readable. Third, the fault-tolerant execution model---with per-domain logging, skip-on-completion, and resume-from-failure---addresses a practical challenge that arises at scale: individual domains may fail due to data gaps, numerical instabilities, or infrastructure interruptions

%% ============================================================================
\section{Large-Domain Application}

The large-sample experiment of Section 9 demonstrated SYMFLUENCE's ability to scale a single model configuration across 111 independent catchments. That experiment, however, treated each catchment as a lumped unit with no lateral routing between sub-basins. For regional water-resource assessment or flood forecasting, a distributed representation---in which precipitation is partitioned across spatially explicit response units and routed through a connected river network---is required. This section evaluates SYMFLUENCE's capacity to configure, execute, and calibrate a fully distributed hydrological simulation over the entire island of Iceland ($\sim$103,000~km$^2$; Figure~\ref{fig:large_domain}).

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/fig_large_domain.pdf}
\caption{Overview of the distributed Iceland domain. (a) GRU mesh coloured by mean elevation, with the delineated river network overlaid. (b) Distribution of GRU areas. (c) Distribution of GRU mean elevations.}
\label{fig:large_domain}
\end{figure}

\subsection{Domain and Experimental Design}

The model domain encompasses the full island of Iceland within the bounding box 63.0--66.5$^\circ$N, 25.0--13.0$^\circ$W. Domain delineation follows the stream-threshold approach (threshold = 2000 grid cells on the Copernicus DEM), with coastal watershed extraction enabled to capture the numerous short, steep rivers that drain directly to the ocean without converging into larger trunk systems. The resulting mesh comprises 6,600 GRUs connected by 6,606 river segments, with elevation-band subdivision yielding 21,474 HRUs.

The hydrological model is FUSE \cite{Clark2008} in distributed spatial mode, with lateral flows routed through the delineated river network using mizuRoute \cite{Gharari2020} at an hourly routing timestep. ERA5 reanalysis provides meteorological forcing at 0.25$^\circ$ spatial and hourly temporal resolution, with a fixed lapse rate correction of 6.5$^\circ$C~km$^{-1}$ applied to air temperature. Thirteen FUSE parameters are calibrated simultaneously using DDS \cite{TolsonShoemaker2007} with a budget of 1,000 iterations and KGE as the objective function.

\subsection{Distributed Configuration}

The distributed configuration differs from the lumped per-catchment setup of Section 9 in three key respects. First, the entire island is represented as a single connected domain rather than 111 independent catchments, so water routed from an upstream GRU enters the downstream GRU through the river network topology. Second, the delineation produces a substantially finer spatial mesh: each GRU corresponds to a sub-basin of the stream network rather than a full gauged catchment. Third, mizuRoute is invoked as an explicit routing component, reading gridded runoff from FUSE and producing reach-level discharge at every timestep.

The SYMFLUENCE configuration for this experiment is specified in a single YAML file that sets \texttt{FUSE\_SPATIAL\_MODE: distributed}, \texttt{ROUTING\_DELINEATION: distributed}, and \texttt{DELINEATE\_COASTAL\_WATERSHEDS: true}. All other workflow steps---DEM acquisition, intersection of forcing grids with the GRU mesh, parameter file generation, model compilation, and output evaluation---proceed through the same automated pipeline used for the single-catchment experiments, with no code modification required.

\subsection{Framework Implications}

This experiment demonstrates that SYMFLUENCE can scale from lumped single-catchment simulations to fully distributed regional domains without changing the underlying model code. The same FUSE executable and mizuRoute binary used in the lumped experiments are reused here; only the configuration file and the spatial inputs differ. This separation of spatial discretisation from model physics is central to the framework's design

%% ============================================================================
\section{Multivariate Evaluation}

Sections 2--10 evaluated hydrological model performance exclusively against observed streamflow. While discharge integrates many watershed processes and remains the most widely available validation target, it provides limited information about the internal consistency of simulated states---a model may reproduce the hydrograph accurately while misrepresenting snow accumulation, soil moisture dynamics, or subsurface storage \cite{Kirchner2006}. Satellite-derived observations of snow cover, soil moisture, and terrestrial water storage now offer spatially distributed constraints on process representations that streamflow alone cannot provide. This section evaluates SYMFLUENCE's capacity to ingest, compare against, and calibrate toward multiple observation sources simultaneously, using three experiments that span different variables, spatial scales, and satellite products (Figure~\ref{fig:multivar}).

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/fig_multivariate.pdf}
\caption{Multivariate evaluation example: Bow at Banff TWS analysis. (a) Monthly TWS anomaly time series comparing GRACE-FO observations against SUMMA simulations. (b) Mean seasonal TWS cycle. (c) Simulated vs.\ observed monthly TWS anomalies.}
\label{fig:multivar}
\end{figure}

\subsection{Experiment (a): GRACE Total Water Storage}

Terrestrial water storage (TWS) anomalies derived from the GRACE and GRACE-FO satellite missions provide a vertically integrated measure of changes in snow, soil moisture, and groundwater---quantities that are poorly constrained by streamflow observations alone. This experiment compares simulated TWS anomalies from SUMMA against GRACE-FO JPL RL06 mascon solutions in the Bow River basin at Banff (2,210~km$^2$).

The domain is configured as a lumped catchment with ERA5 forcing at hourly resolution. The calibrated configuration optimises 14 SUMMA parameters using DDS with 1,000 iterations, targeting a weighted combination of TWS anomaly correlation against GRACE (weight 0.5) and streamflow KGE (weight 0.5). The parameter set includes four groundwater and storage parameters---\texttt{aquiferBaseflowExp}, \texttt{aquiferScaleFactor}, \texttt{specificStorage}, and \texttt{specificYield}---that are not typically included in streamflow-only calibration but are expected to influence TWS dynamics directly.

\subsection{Experiment (b): Joint Snow Cover and Soil Moisture}

Snow cover area (SCA) and soil moisture represent two key state variables that constrain the surface water and energy balance from above and below the land surface, respectively. This experiment jointly evaluates SUMMA simulations against MODIS-derived SCA and SMAP-derived soil moisture in the Paradise Creek catchment on Mt.\ Rainier, Washington ($\sim$500~km$^2$).

MODIS MOD10A1 provides daily binary SCA maps at 500~m resolution, while SMAP Level 3 provides soil moisture retrievals at $\sim$36~km resolution on a 3-day repeat cycle. The multi-objective function combines SCA accuracy (weight 0.5) with soil moisture Pearson correlation (weight 0.5).

\subsection{Experiment (c): Regional Snow Cover Trends}

This experiment evaluates whether SYMFLUENCE can reproduce the spatial pattern and magnitude of snow cover fraction (SCF) trends across Iceland over the 2000--2023 MODIS record. The simulation spans the full MODIS era with CARRA reanalysis forcing at 2.5~km resolution. Trend analysis applies the Mann-Kendall test with Sen's slope estimator, stratified by season and elevation band.

\subsection{Framework Implications}

The multivariate evaluation experiments demonstrate that SYMFLUENCE can ingest heterogeneous observation sources---gravimetric (GRACE), optical (MODIS), and microwave (SMAP)---through a unified handler interface. The weighted multi-objective calibration mechanism enables users to combine arbitrary evaluation metrics into a single objective function through the declarative \texttt{MULTIVAR\_TARGETS} configuration

%% ============================================================================
\section{Parallel Scaling}

SYMFLUENCE implements a three-tier parallel execution architecture that automatically selects among sequential, shared-memory (ProcessPool via \texttt{concurrent.futures}), and distributed-memory (MPI via \texttt{mpi4py}) execution strategies based on the runtime environment and user configuration. Additionally, JAX-based models (HBV, jFUSE) support JIT compilation and GPU offloading, providing model-level acceleration that composes orthogonally with process-level parallelism.

\subsection{Strong Scaling}

To quantify the shared-memory scaling ceiling, we calibrate HBV with DDS (1,000 iterations) while varying the number of ProcessPool workers from 1 to 8 on a laptop and 1 to 64 on a single HPC node. Each worker receives an isolated scratch directory (via SYMFLUENCE's \texttt{LocalScratchManager}) to prevent concurrent file I/O conflicts.

The laptop achieves near-linear speedup up to 4 workers, after which efficiency degrades due to the overhead of per-process directory setup, settings file copying, and result aggregation. The HPC node extends efficient scaling further due to higher memory bandwidth and faster storage I/O, but diminishing returns are observed beyond $\sim$16--32 workers as the serial fraction of the DDS algorithm begins to dominate according to Amdahl's law.

\subsection{MPI Scaling}

The same HBV/DDS problem is repeated using the MPI execution strategy. SYMFLUENCE's \texttt{MPIExecutionStrategy} auto-detects the MPI environment and distributes tasks from the master rank to workers. At low core counts on a single node, ProcessPool outperforms MPI because it avoids inter-process serialisation overhead. However, MPI is the only strategy that scales beyond a single node, making it essential for HPC deployments.

A key observation is the automatic fallback behaviour: when MPI is unavailable, SYMFLUENCE transparently falls back to ProcessPool, and when \texttt{NUM\_PROCESSES = 1}, it falls back to sequential execution. This graceful degradation ensures that the same configuration file can be deployed on both laptop and HPC without modification.

\subsection{JAX Acceleration}

SYMFLUENCE's HBV model supports two computational backends: standard NumPy and JAX. The JAX backend enables JIT compilation and optional GPU offloading. We benchmark four backend configurations: NumPy baseline, JAX CPU without JIT, JAX CPU with JIT, and JAX GPU (HPC only). JAX-JIT provides substantial speedup for the iterative model evaluations required during calibration.

To demonstrate that model acceleration and process-level parallelism are orthogonal, we additionally run JAX-JIT with 1, 4, and 8 ProcessPool workers, computing the compound speedup. The two dimensions compose multiplicatively as expected for embarrassingly parallel calibration with independent model evaluations.

\subsection{Framework Implications}

Three findings bear on practical deployment. First, the automatic execution strategy selection combined with per-process directory isolation enables the same YAML configuration to run unchanged on both a laptop and an HPC cluster. Second, JAX-based model acceleration and process-level parallelism compose multiplicatively. Third, task-level parallelism---simply launching independent experiments concurrently---provides the highest return on investment for the most common usage pattern (multi-model ensembles, multi-seed robustness, multi-algorithm calibration)

%% ============================================================================
\section{Data Processing Pipeline}

The SYMFLUENCE data processing pipeline comprises 16 stages organized as a directed acyclic graph (Figure~\ref{fig:pipeline}). The same pipeline executes identically for all three domains (Paradise, Bow, Iceland), with total output ranging from 12.3~MB (Paradise, 1 HRU) through 132.9~MB (Bow, 49 HRUs) to 4.3~GB (Iceland, 21,474 HRUs).

The CF-Intermediate Format provides the well-defined interface between data acquisition and model execution that makes scale-invariance possible. Weight caching amortizes geometric intersection costs across experiments.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/fig_data_pipeline.pdf}
\caption{Data processing pipeline. (a) Pipeline architecture showing 16 stages organized as a directed acyclic graph with five categories: setup, geospatial, forcing, observations, and model setup. (b) Forcing data transformation for Bow at Banff showing ERA5 grid intersection, weight application, and lapse-rate correction. (c) Observation processing for station data (WSC streamflow) and remote sensing (GRACE TWS).}
\label{fig:pipeline}
\end{figure}

%% ============================================================================
\section{Discussion}

\subsection{From Infrastructure to Inference}

The central argument of this series is that workflow infrastructure is a prerequisite to robust scientific inference in hydrology \cite{Hutton2016, Keshavarz2024}. Consider the multi-model ensemble: six structurally diverse models were configured, calibrated, and evaluated through configuration changes alone. In conventional workflows, assembling such an ensemble demands separate preprocessing scripts for each model, ad hoc output reformatting, and manual alignment of evaluation periods \cite{Knoben2022}. SYMFLUENCE reduces this to configuration changes.

The experiments presented here would be prohibitively labor-intensive using traditional approaches. The 64-member FUSE decision ensemble alone represents 64 separate calibration campaigns, each requiring configuration of model structure, parameter bounds, forcing data, and evaluation periods. The 111-catchment Iceland application multiplies this by two orders of magnitude. The forcing ensemble requires maintaining seven parallel data acquisition and preprocessing pipelines. That these experiments could be completed---and their results compared on equal footing---reflects the architectural investment described in the companion papers.

\subsection{What the Experiments Reveal}

Beyond demonstrating that the architecture works, the experiments yield substantive hydrological findings that bear on ongoing debates in the field.

\textbf{Ensemble averaging works, and we now know why.} The six-model ensemble (KGE = 0.94) exceeds every individual model not through some mysterious emergent property but through the cancellation of offsetting biases \cite{Ajami2006, Arsenault2015}. HBV systematically underestimates peak flows; GR4J overestimates recession rates. In the average, these errors partially neutralize. This finding has practical implications: ensemble construction should prioritize structural diversity over adding more members of similar architecture.

\textbf{Forcing matters more than we acknowledge.} The forcing ensemble reveals that parameter values are not intrinsic model properties but forcing-dependent compensations. The \texttt{frozenPrecipMultip} parameter varies five-fold across forcing datasets, compensating for systematic precipitation biases in each product. Parameters calibrated to one forcing dataset may not transfer to another, even within the same model structure. This has implications for climate change projections that swap historical forcings for future scenarios without recalibrating.

\textbf{Structural uncertainty is underestimated.} The FUSE decision ensemble shows that the KGE spread within a single model framework (2.74) rivals the spread between different models. Structural choices---how to represent percolation, interflow, evaporation---interact in nonlinear ways that simple sensitivity analyses miss. The finding that interactions explain more variance than main effects (35.7\% vs.\ 34.1\%) suggests that model intercomparison studies that vary one structural choice at a time systematically underestimate structural uncertainty.

\textbf{Gradient methods generalize better.} The calibration algorithm comparison found that ADAM---a gradient-based method developed for deep learning---achieved the highest evaluation KGE (0.778) despite ranking fifth in calibration. This suggests that the smoothed HBV formulation required for automatic differentiation acts as an implicit regularizer, preventing overfitting to calibration-period noise. This finding aligns with recent work on differentiable modeling \cite{Shen2018} and suggests that the hydrological community's historical preference for derivative-free methods may warrant reconsideration.

\subsection{Uncertainty Partitioning}

A key finding is that intra-model structural uncertainty (Section 7, KGE spread of 2.74 across 64 FUSE structures) rivals inter-model uncertainty (Section 3). Structural decisions do not act independently: interactions explain 35.7\% of KGE variance versus 34.1\% for main effects.

This has implications for how the community reports model uncertainty. Studies that compare ``SUMMA vs.\ VIC vs.\ HYPE'' capture inter-model uncertainty but implicitly assume that structural choices within each model are optimal or representative. The FUSE experiment shows this assumption is questionable: the ``best'' FUSE configuration (KGE = 0.86) differs from the ``worst'' (KGE = $-$1.89) only in six binary decisions. A more complete uncertainty quantification would sample both within-model and between-model structural spaces.

\subsection{Scale Invariance in Practice}

The progression from Paradise SNOTEL (1 HRU) through Bow at Banff (49 GRUs) to Iceland (21,474 HRUs) demonstrates that SYMFLUENCE's scale-invariant design works in practice. The same configuration schema, the same workflow engine, and the same evaluation metrics apply across three orders of magnitude in domain complexity. The only changes are spatial specifications (pour point vs.\ bounding box) and computational resources.

This scale invariance has practical implications for model development workflows. A researcher can prototype and debug on a small domain (fast iteration, immediate feedback), then deploy to regional scale without rewriting code or adapting workflows. The CF-Intermediate Format ensures that forcing data, model outputs, and evaluation metrics maintain consistent semantics across scales.

\subsection{Limitations}

Several limitations qualify these conclusions. First, the experiments concentrated on a small number of domains (primarily Bow at Banff and Iceland), limiting the generalizability of quantitative findings. The finding that ensemble averaging improves KGE by 0.06 over the best individual model may not hold in all climatic regimes.

Second, evaluation periods were relatively short (3--7 years), which may not capture low-frequency hydrological variability or rare events. The sensitivity analysis, in particular, reflects parameter importance for reproducing ``typical'' years rather than extremes.

Third, most calibration used single-objective KGE against streamflow. The multivariate evaluation experiments (Section 11) demonstrate that SYMFLUENCE supports multi-objective calibration, but the other experiments did not exploit this capability. Calibrating to streamflow alone may produce parameters that reproduce the hydrograph while misrepresenting internal states \cite{Kirchner2006}.

Fourth, model integration depth varies across the six-model ensemble. SUMMA and FUSE are deeply integrated with full parameter control; the LSTM wrapper provides less granular access to internal states. This heterogeneity complicates direct comparison.

Finally, the framework's complexity itself is a limitation. SYMFLUENCE requires a substantial initial investment to learn and deploy. The experiments presented here were conducted by the framework's developers; whether external users can achieve similar productivity remains to be demonstrated.

%% ============================================================================
\section{Conclusion}

This paper has subjected the architectural principles described in the companion papers to empirical test through twelve experiments. The experiments are not definitive hydrological studies; their purpose is to demonstrate that when technical friction is reduced through deliberate architectural design, experiments that would otherwise be prohibitively labor-intensive become routine.

The multi-model ensemble showed that six models can be configured through configuration changes alone, with the ensemble mean exceeding every individual model. The forcing ensemble revealed forcing-dependent parameter compensation. The algorithm comparison identified bimodal parameter regimes. The structural decision ensemble showed that interactions dominate main effects. The sensitivity analysis identified soil storage and routing as consistently influential. The data pipeline analysis characterized processing across three orders of magnitude in domain size.

Whether SYMFLUENCE ultimately contributes to resolving predictive stagnation depends on how it is used \cite{Nearing2021}. The framework removes technical barriers; it does not generate scientific hypotheses or interpret results \cite{EythorssonClark2025}. Its contribution is to shift the bottleneck from implementation to inquiry.

%% ============================================================================
\section*{Open Research}
The SYMFLUENCE framework is available at [repository URL]. Configuration files for all experiments are provided in the supplementary material. The LamaH-Ice dataset is available from [reference]. ERA5 data are available from the Copernicus Climate Data Store.

%% ============================================================================
\acknowledgments
[Acknowledgments to be added.]

%% ============================================================================
\bibliography{references}

\end{document}
