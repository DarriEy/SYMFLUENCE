## 4.3 Forcing ensemble – Paradise

Whereas Section 4.2 held the forcing dataset fixed and varied model structure, this experiment holds model structure fixed and varies the atmospheric forcing. Using SUMMA at the Paradise SNOTEL station in Washington State (46.78°N, 121.75°W; 1,561 m a.s.l.), we calibrated an identical set of 11 snow-related parameters against observed daily SWE under each of 14 forcing datasets. The calibration period spans water years 2016–2018 (October 2015 – September 2018), with independent evaluation over water years 2019–2020 (October 2018 – September 2020). All experiments used DDS optimisation of RMSE against SNOTEL SWE, with configuration files differing only in their forcing data paths and identifiers.

The 14 forcing datasets fall into two categories: four observation-based reanalysis or gridded products — ERA5 (~31 km; Hersbach et al., 2020), AORC (~1 km; NOAA, 2022), CONUS404 (~4 km; Rasmussen et al., 2023), and RDRS (~10 km; Environment and Climate Change Canada) — and ten members of the NEX-GDDP-CMIP6 downscaled climate projections (~25 km; Thrasher et al., 2022): ACCESS-CM2, CanESM5, CNRM-CM6-1, GFDL-ESM4, INM-CM5-0, IPSL-CM6A-LR, MPI-ESM1-2-HR, MRI-ESM2-0, NorESM2-LM, and UKESM1-0-LL. The reanalysis products span a 30-fold range in horizontal resolution, while the ten GDDP members sample the structural diversity of global climate models applied to a common statistical downscaling framework. Table 15 summarises the forcing datasets and their calibration performance.

> **Table 15.** Summary of forcing datasets and SUMMA performance at Paradise SNOTEL. All configurations calibrate 11 parameters against observed SWE using DDS.

### 4.3.1 SWE Time Series

Figure 1 presents the simulated and observed SWE over the full 2015–2020 period. The upper panel (Figure 1a) compares the four reanalysis-driven simulations against observed SNOTEL SWE; the lower panel (Figure 1b) shows the ten GDDP-driven simulations, their ensemble envelope, and ensemble mean. Calibration and evaluation periods are indicated by shading.

> **Figure 1.** Simulated and observed snow water equivalent (SWE) at Paradise SNOTEL (2015–2020). (a) Reanalysis-driven simulations (ERA5, AORC, CONUS404, RDRS). (b) GDDP-driven simulations (10 CMIP6 members) with ensemble envelope and mean. Shading indicates calibration and evaluation periods.

All reanalysis-driven simulations reproduce the dominant seasonal SWE cycle, with accumulation from November through April and melt completing by July. However, the spread among reanalysis products is substantial. ERA5-driven SWE consistently overshoots observed peak SWE by 500–1,000 mm during the calibration period, and by over 1,500 mm in the evaluation winters of 2019 and 2020. This overshoot reflects a known positive precipitation bias in ERA5 at high-elevation Pacific Northwest sites, which the calibration cannot fully absorb through the frozen precipitation multiplier alone (calibration RMSE = 649 mm, evaluation RMSE = 1,366 mm). AORC (~1 km) and CONUS404 (~4 km) track the observed record most faithfully, with AORC producing the closest match to both peak magnitude and melt timing across all five snow seasons (evaluation KGE = 0.87). RDRS captures the general pattern well during calibration (KGE = 0.94) but underestimates peak SWE in the evaluation period, with a negative bias of 30 mm and an evaluation KGE of 0.60.

The ten GDDP-driven simulations (Figure 1b) display considerably wider inter-member spread than the reanalysis ensemble, particularly during accumulation seasons. Calibration KGE across the ten members ranges from 0.49 (NorESM2-LM) to 0.82 (CanESM5), with a mean of 0.67 — comparable to the midrange of the reanalysis products. The ensemble envelope expands noticeably during evaluation, consistent with the fact that GDDP products represent climate model free runs rather than observation-constrained reanalyses. Evaluation KGE spans a wide range, from 0.08 (CNRM-CM6-1) to 0.81 (NorESM2-LM), reflecting the heterogeneous quality of the underlying GCM precipitation and temperature fields at this maritime mountain site.

Several individual member behaviours are noteworthy. ACCESS-CM2 and CanESM5 achieve high calibration skill (KGE = 0.76 and 0.82, respectively) but degrade substantially in evaluation (KGE = 0.37 and 0.40), suggesting that calibration compensates for systematic GCM biases that shift between the two periods. Conversely, GFDL-ESM4 and MRI-ESM2-0 improve from calibration to evaluation (KGE from 0.58 to 0.72 and from 0.65 to 0.69, respectively), indicating more temporally stable forcing error structures. CNRM-CM6-1 presents the most extreme degradation among all 14 forcings (KGE from 0.71 to 0.08), with an evaluation bias of −470 mm — the largest of any ensemble member — pointing to a pronounced shift in its precipitation climatology between the two periods. MPI-ESM1-2-HR, despite strong calibration performance (KGE = 0.80), degrades sharply in evaluation (KGE = 0.25), paralleling the pattern seen with RDRS where high calibration skill achieved through parameter compensation fails to generalise. The ensemble mean provides a reasonable central estimate during calibration but does not systematically outperform the better individual members, consistent with the asymmetric error distributions across GCMs at this site.

### 4.3.2 Performance and Transferability

Figure 2 quantifies simulation performance across the 14 forcings. The heatmap (Figure 2a) reports KGE, correlation, NSE, and RMSE for both calibration and evaluation periods. The bar chart (Figure 2b) displays KGE degradation — defined as the difference between calibration and evaluation KGE — as a measure of parameter transferability.

> **Figure 2.** SUMMA SWE simulation performance across 14 forcing datasets. (a) Heatmap of KGE, correlation, NSE, and RMSE for calibration and evaluation periods. (b) KGE degradation between calibration and evaluation as a measure of parameter transferability.

Two key patterns emerge from the expanded 14-member ensemble. First, calibration performance is not a reliable predictor of evaluation performance. RDRS achieves the highest calibration KGE of any forcing (0.94) but degrades by 0.35 to an evaluation KGE of 0.60, while AORC, with a more modest calibration KGE (0.77), actually improves during evaluation (KGE = 0.87, degradation = −0.10). This counter-intuitive result suggests that AORC's higher spatial resolution (1 km) provides forcing fields that are more physically representative of the point-scale SNOTEL site, allowing the calibrated parameter set to generalise without overfitting. RDRS and ERA5, by contrast, deliver forcing at 10–31 km resolution that requires calibration to compensate for systematic biases — a compensation that transfers poorly to unseen years.

This pattern is strongly reinforced by the GDDP ensemble. Four of the 14 forcings actually improve from calibration to evaluation: AORC (−0.10), GFDL-ESM4 (−0.14), MRI-ESM2-0 (−0.05), and NorESM2-LM (−0.32). NorESM2-LM is particularly striking: it has the lowest calibration KGE of any GDDP member (0.49), yet achieves the highest evaluation KGE in the entire ensemble (0.81), surpassing all reanalysis products except AORC. Conversely, CanESM5 (calibration KGE = 0.82) and MPI-ESM1-2-HR (0.80) — the two highest-performing GDDP members in calibration — both degrade substantially (to 0.40 and 0.25, respectively). Across the full GDDP ensemble, mean calibration KGE is 0.67 (σ = 0.11), while mean evaluation KGE drops to 0.46 (σ = 0.24), with the tripling of spread indicating that forcing biases manifest more heterogeneously in the independent evaluation period.

Second, ERA5 remains a clear outlier. Its calibration KGE of 0.30 already indicates poor fit, and the evaluation KGE collapses to −0.59 (degradation = +0.89), the largest of any forcing. The evaluation RMSE of 1,366 mm is roughly four times that of AORC and more than double that of any other forcing, reflecting persistent and amplifying precipitation biases at this maritime mountain site. CNRM-CM6-1 exhibits the second-largest degradation (+0.64), driven by an evaluation bias of −470 mm that dwarfs its calibration bias of −14 mm, pointing to a non-stationary precipitation error structure in the underlying GCM.

Calibration correlation remains above 0.59 for all 14 forcings, indicating that seasonal timing is consistently captured regardless of forcing quality; the primary drivers of KGE variation are bias and variability ratio, consistent with the Section 4.2 decomposition findings. Evaluation correlation is more variable, ranging from 0.38 (ACCESS-CM2) to 0.98 (RDRS), with RDRS maintaining excellent timing despite its degradation in overall KGE — confirming that its evaluation shortcomings arise from bias and amplitude errors rather than phase shifts.

### 4.3.3 Parameter Compensation and Equifinality

Figure 3 examines how the calibrated parameter values vary across the 14 forcings and whether systematic compensation patterns emerge. The heatmap (Figure 3a) displays Z-score-normalised parameter values for all forcings (rows sorted by KGE degradation), with raw values annotated. The lower panels show the frozen precipitation multiplier (frozenPrecipMultip) against KGE degradation (Figure 3b) and a composite distortion index against KGE degradation (Figure 3c).

The parameter heatmap reveals pronounced forcing-dependent compensation across all 14 configurations. The frozenPrecipMultip parameter — a direct multiplier on solid precipitation — spans a six-fold range, from 0.84 (CONUS404) to 4.98 (MRI-ESM2-0). Forcings with known precipitation biases at the study site drive the calibration towards extreme multiplier values to compensate. Among the reanalysis products, ERA5 and RDRS calibrate to multipliers of 1.71 and 4.08, respectively, while AORC and CONUS404 require multipliers near unity (2.97 and 0.84), indicating that their finer resolutions better resolve orographic precipitation gradients and already provide physically reasonable precipitation amounts at the point scale. The GDDP members span the full range of the multiplier, from 2.50 (MPI-ESM1-2-HR) to 4.98 (MRI-ESM2-0), reflecting the diverse precipitation biases inherent in the underlying GCMs.

A similar pattern appears in the melt-related parameters. The melt exponent (mw_exp) ranges from 1.05 (CNRM-CM6-1) to 5.00 (CanESM5 and INM-CM5-0), spanning nearly the full feasible parameter range. The albedo decay rate (albedoDecayRate) varies by nearly two orders of magnitude, from ~100,000 (NorESM2-LM) to ~4,900,000 (GFDL-ESM4). These compensatory adjustments suggest that the calibration is not recovering the "true" snow physics but rather absorbing forcing biases into parameter values that are tuned to specific error structures. CNRM-CM6-1 is notable for combining a moderate precipitation multiplier (3.01) with the lowest melt exponent in the ensemble (1.05), suggesting that its calibration compensates primarily through melt dynamics rather than precipitation adjustment — a strategy that proves brittle in evaluation (KGE degradation = +0.64).

> **Figure 3.** Forcing-dependent parameter divergence across 14 datasets. (a) Z-score-normalised calibrated parameter values with raw annotations. (b) Frozen precipitation multiplier versus KGE degradation. (c) Composite distortion index versus KGE degradation.

The scatter plots (Figure 3b–c) formalise this relationship. Figure 3b shows that the frozen precipitation multiplier alone explains limited variance in KGE degradation (R² = 0.15, p = 0.18), though the trend is positive — forcings requiring larger multipliers tend to degrade more. The relationship is weakened by cases such as MRI-ESM2-0 (multiplier = 4.98, degradation = −0.05) and NorESM2-LM (multiplier = 4.63, degradation = −0.32), where high multiplier values are paired with stable or improving evaluation performance, suggesting that other parameter adjustments buffer the effect. Figure 3c aggregates three compensatory parameters (frozenPrecipMultip, tempRangeTimestep, mw_exp) into a composite distortion index (mean absolute Z-score). Even with the expanded 14-member ensemble, the regression between composite distortion and KGE degradation is not statistically significant (R² < 0.01, p = 0.88), indicating that parameter distortion is a necessary but not sufficient condition for poor transferability. The lack of a clear linear relationship reflects the multivariate nature of parameter compensation: different forcings induce distortion in different parameter subsets, and the interactions among compensatory adjustments determine whether the overall configuration generalises. This finding argues against using any single summary metric of parameter deviation as a proxy for forcing quality and instead supports the multi-metric diagnostic approach adopted here.

### 4.3.4 Framework Implications

The forcing ensemble experiment highlights three aspects of SYMFLUENCE's design.

First, the experiment demonstrates that SYMFLUENCE's configuration-driven approach makes it straightforward to isolate the effect of a single experimental factor. All 14 experiments share an identical YAML configuration except for the forcing dataset specification (dataset name, model identifier, and data path). The ten GDDP members were generated from a single template configuration using automated substitution of model identifiers, requiring no changes to the core framework. This degree of control would be difficult to achieve with ad hoc scripting workflows, where forcing-specific preprocessing steps are typically hard-coded.

Second, the results provide a practical caution against interpreting calibration skill as a measure of forcing quality. RDRS achieves the highest calibration KGE (0.94), yet its calibrated parameters include extreme values of the precipitation multiplier and melt exponent that fail to generalise. NorESM2-LM, with the lowest GDDP calibration KGE (0.49), produces the best GDDP evaluation performance (KGE = 0.81) and parameter values closer to their physically expected ranges. Similarly, AORC, with its lower calibration KGE (0.77), transfers successfully to the evaluation period (KGE = 0.87). This finding reinforces the importance of split-sample evaluation — which SYMFLUENCE automates through its configuration of distinct calibration and evaluation periods — when comparing forcing datasets.

Third, the completed ten-member GDDP-CMIP6 ensemble demonstrates SYMFLUENCE's extensibility to climate projection workflows. The resulting forcing uncertainty envelope — with evaluation KGE ranging from 0.08 to 0.81 across ten GCMs — can be directly compared with the structural uncertainty envelope from Section 4.2, enabling a partitioning of the total predictive uncertainty into its forcing and structural components. The substantial inter-model spread (evaluation KGE σ = 0.24) underscores that forcing uncertainty is a first-order contributor to predictive uncertainty at snow-dominated sites, a finding with direct implications for the design of climate impact assessments that rely on hydrological model chains.
