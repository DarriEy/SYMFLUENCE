% Outstanding in Every Field - WRR Submission
% AGU Journal Template for Water Resources Research
\documentclass[draft,wrcr]{agujournal2019}

% Required packages
\usepackage{apacite}
\usepackage{lineno}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{url}

% Line numbers for review
\linenumbers

% Draft watermark (remove for final)
\draftfalse  % Set to \drafttrue for draft watermark

\begin{document}

%% Title
\title{Outstanding in Every Field: The Case for Shared Architectural Vision in Computational Hydrology}

%% Authors - Update affiliations as needed
\authors{
Darri Eythorsson\affil{1},
% [Add co-authors here]
Martyn Clark\affil{1}
}

%% Affiliations
\affiliation{1}{Centre for Hydrology, University of Saskatchewan, Saskatoon, Saskatchewan, Canada}
% Add additional affiliations as needed
% \affiliation{2}{Institution, City, Country}

%% Corresponding author
\correspondingauthor{Darri Eythorsson}{darri.eythorsson@usask.ca}

%% Key Points (exactly 3 required, max 140 characters each)
\begin{keypoints}
\item Modern computational hydrology demands expertise across too many technical domains for any individual researcher to master
\item Predictive stagnation reflects infrastructure fragmentation rather than scientific limits; the problem is architectural, not intellectual
\item Registry-based, declarative infrastructure enables community coordination without central gatekeeping or individual heroism
\end{keypoints}

%% ============================================================================
%% ABSTRACT
%% ============================================================================
\begin{abstract}
Modern computational hydrology demands that its practitioners simultaneously master process understanding across the terrestrial water cycle, navigate legacy codebases in Fortran and C, orchestrate large-data workflows in Python, R, and shell, understand optimization theory, operate high-performance computing infrastructure, and remain conversant with advances in machine learning, remote sensing, and GPU-accelerated computation---all while the discipline rightly insists that credible hydrologists must also maintain connection to field observation and physical intuition. No individual can sustain expertise across this full spectrum, yet the field's infrastructure implicitly assumes they will. This paper argues that the resulting fragmentation---not any failure of hydrological science itself---is a primary cause of the predictive stagnation observed over the past two decades. We propose that the path forward lies not in demanding more from individual researchers but in adopting shared architectural principles that allow the community to compose, extend, and build upon each other's technical contributions as naturally as it builds upon each other's scientific ideas. Drawing on lessons from software engineering and other computational sciences, we outline a set of design principles---declarative specification, registry-based extensibility, provenance by default, and separation of scientific intent from computational implementation---that could transform hydrological modeling from a craft practice into a cumulative engineering discipline. The problem is not a lack of talent or effort. The problem is that we have asked each generation to rebuild the scaffolding before they can begin the science.
\end{abstract}

%% ============================================================================
%% PLAIN LANGUAGE SUMMARY (required, max 200 words)
%% ============================================================================
\section*{Plain Language Summary}
Today's computational hydrologist is expected to be an expert in everything: understanding how water moves through landscapes, programming in multiple languages, managing massive datasets, running supercomputers, and keeping up with machine learning---while also maintaining the field experience that grounds good science. No one can truly master all of this, yet our tools assume everyone will. This impossible expectation fragments the field: each research group builds its own data processing scripts, its own model wrappers, its own workarounds. Graduate students spend months learning to compile software before testing their first hypothesis. Published studies cannot be reproduced because the ``glue code'' connecting their pieces was never shared. We argue this infrastructure problem---not any limit of hydrological science---explains why prediction skill has plateaued. The solution is not to demand more from individuals but to adopt shared design principles from software engineering: standard interfaces between tools, registries that let new components plug in without rewriting everything, and configurations that separate scientific choices from technical details. These principles would let specialists contribute their expertise without needing to understand the entire system, transforming hydrology from a craft where each generation rebuilds the scaffolding into a discipline where contributions accumulate.

%% ============================================================================
%% MAIN TEXT
%% ============================================================================

\section{Introduction}

The hydrologist of 1970 needed to understand rainfall-runoff processes, read a rain gauge, and perhaps run a simple water balance model on a mainframe. The job description has expanded somewhat since then. The hydrologist of 2025 is expected to do all of that and also: acquire and preprocess terabytes of satellite imagery and reanalysis data across dozens of formats and APIs; compile, configure, and debug process-based models written in Fortran 77, Fortran 90, C, and C++; write preprocessing and analysis workflows in Python, R, MATLAB, Julia, or some combination thereof; understand and implement optimization algorithms spanning evolutionary computation, gradient-based methods, Bayesian inference, and machine learning; deploy simulations on high-performance computing clusters using SLURM, MPI, and containerized environments; interpret results from remote sensing platforms with their own quality flags, orbital geometries, and retrieval uncertainties; and increasingly, train, validate, and explain neural network architectures that challenge the primacy of process-based reasoning.

They should also, ideally, know what a stream looks like.

The breadth of technical competence now required to conduct state-of-the-art computational hydrology has expanded far beyond what any individual can reasonably master. This is not a complaint about the difficulty of the work---hydrologists have always faced hard problems \cite{Dooge1986, BloschlSivapalan1995}. It is an observation about the structure of the discipline's technical infrastructure: we have accumulated powerful tools across every stage of the modeling lifecycle \cite{Bierkens2015, Wood2011, McCabe2017}, but we have not invested proportionally in the connective architecture that would allow these tools to be composed, shared, and reused without each researcher first becoming an expert in their implementation details.

The consequences are visible across the field. Graduate students spend months learning to compile models before they can test a scientific hypothesis. Research groups maintain private collections of preprocessing scripts that duplicate effort across institutions. Published studies describe workflows that cannot be reproduced because the glue code connecting their components was never documented, versioned, or shared \cite{Hutton2016, Baker2016}. And perhaps most consequentially, the field has observed what appears to be a plateau in predictive skill \cite{AddorMelsen2019, Nearing2021}---a stagnation that some attribute to fundamental limits of process-based modeling but that may equally reflect the practical limits of actionably deploying process knowledge through fragmented, artisanal workflows.

This paper argues that the discipline's infrastructure problem is not incidental to its scientific challenges but central to them. The community has articulated grand visions for the future of hydrology \cite{Wagener2010, Bloschl2019} and identified twenty-three unsolved problems---yet many of these remain unsolved not for lack of scientific insight but for lack of infrastructure to test competing hypotheses systematically. We do not lack models, data, algorithms, or computational power. We lack the architectural vision to connect them. And until we address that gap, we cannot know whether predictive stagnation reflects the boundaries of our science or merely the boundaries of our engineering.

%% ============================================================================
\section{The Impossible Generalist}

\subsection{An Archaeology of Required Expertise}

To appreciate the scope of what is now expected, consider the technical layers a researcher must traverse to conduct a single, moderately ambitious hydrological modeling study---say, calibrating a process-based model against streamflow and satellite observations for a mesoscale catchment. Figure~\ref{fig:expertise_layers} maps these layers visually; the paragraphs that follow excavate each one.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/fig1_expertise_layers.pdf}
\caption{The Impossible Generalist: layers of required expertise in modern computational hydrology, from field observation through process understanding, mathematical foundations, legacy code, data science, parameter estimation, optimization, differentiable modeling, machine learning, HPC, and reproducibility infrastructure. Each layer is individually learnable; the compound expectation that a single researcher will master all of them simultaneously is the structural problem this paper identifies.}
\label{fig:expertise_layers}
\end{figure}

\textbf{Process understanding.} The researcher must select and configure representations for snow accumulation and melt, canopy interception, infiltration, soil moisture dynamics, evapotranspiration, surface and subsurface runoff generation, groundwater recharge, and channel routing. Each process admits multiple plausible formulations with distinct assumptions, parameters, and sensitivities \cite{Clark2011, Fenicia2011, HrachowitzClark2017}. Making informed choices requires familiarity with decades of process-based literature spanning soil physics, plant physiology, micrometeorology, and open-channel hydraulics---the intellectual tradition that stretches from \citeA{FreezeHarlan1969} through \citeA{Beven1989} and \citeA{PaniconiPutti2015} to the distributed process-based reviews of \citeA{Fatichi2016}. This is, notionally, what the hydrologist was trained for. It is also, increasingly, the smallest part of the job.

\textbf{Mathematical and numerical foundations.} Beneath the process representations lies a demanding mathematical substrate. The governing equations of land-surface hydrology---Richards' equation for variably saturated flow, the energy balance at the land-atmosphere interface, the Saint-Venant equations for channel routing---are nonlinear partial differential equations that must be derived, discretized, and solved with careful attention to numerical stability, mass conservation, and computational efficiency. A competent modeler should be able to derive the three-dimensional form of Richards' equation on paper, understand why different discretization schemes (finite difference, finite element, finite volume) produce different numerical behaviors, and diagnose when a solver fails because of ill-conditioning rather than because of a bug.

The demands have recently escalated further. The emergence of differentiable modeling \cite{Shen2018, Feng2022, Tsai2021}---in which gradients of model outputs with respect to parameters are computed automatically for use in gradient-based optimization and hybrid physics-ML architectures---requires that these same equations be implemented not only in legacy Fortran solvers but also in modern automatic differentiation frameworks such as JAX, Julia's Zygote, or PyTorch. The researcher must now trace correct flux derivatives through the entire land-surface column, from radiative transfer at the top of the canopy through stomatal conductance and transpiration, through the Richards equation in the vadose zone, to baseflow generation at the water table---and ensure that these derivatives are numerically stable, physically meaningful, and computationally tractable.

\textbf{Legacy numerical codes.} The dominant process-based models---SUMMA \cite{Clark2015a, Clark2015b}, VIC \cite{Liang1994}, MESH \cite{Pietroniro2007}, HYPE \cite{Lindstrom2010}, Noah-MP \cite{Niu2011}, and their predecessors---are implemented in Fortran and C. These are powerful, performant languages that have aged with the quiet dignity of load-bearing walls: you cannot remove them, but renovating around them requires specialized knowledge that fewer practitioners possess each year. Implicit typing conventions, fixed-format source files, platform-specific build dependencies, and debugging workflows that predate the existence of Stack Overflow---these are the daily realities of process-based modeling. Compiling SUMMA on a new system, for instance, requires resolving dependencies on NetCDF-Fortran, LAPACK, and BLAS libraries whose installation varies by operating system, compiler vendor, and HPC module environment. A researcher who cannot navigate Makefiles, linker errors, and Fortran namelist conventions cannot run the model at all, regardless of their hydrological insight. The model does not care about your h-index.

\textbf{Data science at scale.} Forcing a model requires acquiring gridded meteorological data---ERA5, AORC, NLDAS, RDRS, CHIRPS, or one of dozens of alternatives---each with its own API, file format, variable naming convention, unit system, spatial grid, and temporal resolution. Evaluation requires ingesting satellite products (GRACE, MODIS, SMAP, Sentinel, Landsat) and in situ observations (USGS, WSC, GRDC, SNOTEL, FLUXNET) through yet more APIs and formats. The preprocessing pipeline---spatial remapping, temporal alignment, unit conversion, quality filtering, gap filling---is typically implemented in Python using NumPy, Pandas, Xarray, Rasterio, GDAL, and GeoPandas, with shell scripts for orchestration and R for statistical analysis because apparently no single language has yet achieved total dominion over the data science stack. Each of these libraries has its own learning curve, version dependencies, and failure modes. The pipeline is rarely fewer than several thousand lines of code, almost never reused across projects, and seldom documented beyond the author's own memory.

\textbf{Parameters and their meaning.} Before one can calibrate a model, one must understand what its parameters represent---and this understanding spans a conceptual spectrum that is itself a substantial intellectual challenge. At one end sit the physically meaningful parameters of process-based models: saturated hydraulic conductivity, porosity, field capacity, wilting point, albedo decay rates, stomatal resistance coefficients. These are not abstract tuning knobs but representations of the storage and transmission properties of real landscapes. Their values can, in principle, be measured in the field or derived from soil surveys and land-cover databases, and they carry physical units that constrain their plausible ranges. At the other end sit the parameters of conceptual and data-driven models: the shape parameter of a soil moisture curve in HBV, the routing store capacity in GR4J, the learned weights of an LSTM cell \cite{Kratzert2019}. These are tunable representations that may correlate with physical properties but are not directly measurable. Beven's equifinality thesis \cite{Beven2006, BevenFreer2001} showed that multiple parameter sets can produce equally acceptable simulations---a finding that \citeA{Kirchner2006} sharpened by asking whether we are getting the right answers for the right reasons. The question remains open, in part because the infrastructure to systematically investigate it across models, scales, and catchments has not existed.

\textbf{Optimization and calibration.} Given this parameter landscape, the researcher must then choose how to search it. This requires selecting among local search methods (Nelder-Mead, L-BFGS), global methods (DDS: \citeA{TolsonShoemaker2007}; SCE-UA: \citeA{Duan1992}; differential evolution, particle swarm), multi-objective approaches (NSGA-II, MOEA/D), Bayesian methods (DREAM: \citeA{Vrugt2009}; GLUE: \citeA{BevenBinley1992}), and increasingly, gradient-based methods enabled by automatic differentiation. Each algorithm has hyperparameters, convergence properties, and failure modes that require understanding the optimization literature---a field unto itself. The researcher must also define objective functions \cite{NashSutcliffe1970, Gupta2009} and ideally follow the split-sample testing philosophy that \citeA{Klemes1986} advocated four decades ago but that remains inconsistently practiced.

\textbf{High-performance computing.} Meaningful simulation campaigns---ensemble calibrations, multi-model comparisons, large-domain runs---require HPC resources \cite{Kollet2010, Maxwell2015, Kuffour2020}. This introduces SLURM job scripts, MPI parallelization, shared filesystem I/O patterns, memory management, queue policies, and the perpetual negotiation between wall-clock limits and computational ambition.

\textbf{Machine learning and GPU computing.} The rapid adoption of deep learning in hydrology \cite{Shen2018, Kratzert2019, Nearing2021} has added yet another layer: researchers are now expected to understand recurrent neural networks, attention mechanisms, graph neural networks, loss function design, regularization, and the hardware-specific considerations of GPU training.

\textbf{And still, the field.} Through all of this, the discipline maintains---rightly \cite{Savenije2009, Kirchner2006}---that credible hydrological science requires physical intuition grounded in observation. Understanding why a model fails requires knowing what a snowpack looks like in spring, how a prairie pothole fills and drains, or why a particular soil horizon matters for infiltration. Field experience is not a luxury but a foundation. Yet the time consumed by technical infrastructure leaves progressively less room for the muddy boots and frozen fingers that produce it.

\subsection{The Compounding Cost of Breadth}

The issue is not that any single layer is unreasonably difficult. Deriving Richards' equation is a graduate course. Understanding parameter semantics is what advisors are for. Fortran compilation is learnable. Python data pipelines are tractable. HPC job scheduling can be mastered. The issue is the \textit{compound} expectation: that each researcher will independently acquire working proficiency across all layers before they can begin addressing their actual scientific question.

This expectation has three corrosive effects.

\textbf{First, it raises the barrier to entry.} A graduate student beginning a computational hydrology project faces months of technical onboarding before producing their first scientific result. Much of this time is spent solving problems that have been solved many times before by other researchers in other groups, but whose solutions were never externalized into shared infrastructure.

\textbf{Second, it fragments effort.} Because technical solutions remain private to research groups, the community collectively invests enormous effort in duplicated work. Every group that uses SUMMA has written its own preprocessing pipeline. Every group that calibrates with DDS has implemented its own parallel evaluation wrapper. These parallel efforts do not accumulate.

\textbf{Third, it distorts scientific priorities.} When the cost of deploying a complex model is high, researchers rationally favor simpler approaches---not because simpler models are scientifically preferable, but because the technical overhead of the alternative is prohibitive \cite{PappenbergerBeven2006, Schuwirth2019}. The apparent finding that complex models do not consistently outperform simple ones \cite{AddorMelsen2019, SchaefliGupta2007} may partly reflect this selection effect: complex models are more likely to be misconfigured, undertested, and inadequately calibrated, precisely because the infrastructure for deploying them properly does not exist as shared community resources.

\subsection{A Generational Cycle}

Perhaps the most troubling aspect of this situation is its persistence. The problems described above are not new. \citeA{Hutton2016} documented the reproducibility crisis in hydrology a decade ago. \citeA{Peng2011} and \citeA{Stodden2016} raised parallel alarms across computational science more broadly. The need for shared modeling infrastructure has been articulated repeatedly \cite{Clark2015a, Knoben2022, Keshavarz2024}. \citeA{Wilson2014, Wilson2017} published detailed best-practice guides for scientific computing that remain aspirational rather than standard in hydrology. Yet the fundamental pattern remains: each generation of computational hydrologists inherits powerful but disconnected tools and must independently forge the connections between them.

This is not because the community lacks awareness or effort. It is because the problem is genuinely difficult and because the incentive structures of academic research do not reward infrastructure investment. Building a shared preprocessing pipeline is less publishable than a novel model formulation. Maintaining a community build system is less fundable than a scientific discovery. No one has ever received a best paper award for a well-designed Makefile, however many hours of collective suffering it may have prevented.

\subsection{Voices from the Cycle}

The preceding sections describe the generalist burden in structural terms. But structures are inhabited by people, and the cost is ultimately personal. The following reflections, contributed by co-authors at different career stages and institutions, illustrate how the same pattern manifests across contexts.

\begin{quote}
\textbf{[Co-author A---Senior researcher, 20+ years]}\\
\textit{[Placeholder for reflection on how the technical stack has expanded over a career. What could be accomplished in 1998 with Fortran and a shell script vs.\ what is expected now. The experience of watching successive cohorts of students spend their first year on the same infrastructure problems.]}
\end{quote}

\begin{quote}
\textbf{[Co-author B---Early-career researcher / postdoc]}\\
\textit{[Placeholder for reflection on the onboarding experience. How long before the first scientific result? What fraction of working time is spent on science vs.\ plumbing? The feeling of solving a problem that you suspect hundreds of others have solved before you, in isolation.]}
\end{quote}

\begin{quote}
\textbf{[Co-author C---HPC / research software engineer]}\\
\textit{[Placeholder for reflection from the infrastructure side. The experience of supporting researchers who need to compile, deploy, and parallelize codes they did not write. The gap between what models assume about their execution environment and what actual environments provide. The invisibility of infrastructure work in academic incentive structures.]}
\end{quote}

\begin{quote}
\textbf{[Co-author D---Field hydrologist who also models]}\\
\textit{[Placeholder for reflection on the tension between field and computation. How computational demands have encroached on field time. The worry that models are being built by people who have never seen the systems they simulate---and that field scientists are being excluded from modeling because the technical barrier is too high.]}
\end{quote}

These are not complaints about hard work. Every discipline involves effort. They are observations about \textit{misallocated} effort---about talented scientists spending their finite working years on problems that are genuinely important but should not need to be solved by each individual independently.

%% ============================================================================
\section{What Software Engineering Learned (That We Haven't)}

The challenges facing computational hydrology are not unique to hydrology. Every computational science eventually confronts the tension between component diversity and system integration. What distinguishes fields that have navigated this tension successfully is not superior individual effort but the adoption of shared architectural principles that allow independently developed components to compose into larger systems.

\subsection{The Registry Pattern as Governance}

Consider the problem of extensibility: how does a system accommodate new components---models, data sources, algorithms---without requiring modification to its core? In software engineering, this is a solved problem. The registry pattern allows components to declare their capabilities and register themselves with a central catalog at runtime. New components participate in the system by conforming to a defined interface and announcing their presence; existing components need not be aware of them.

This pattern has profound implications for scientific communities. A registry-based architecture means that a research group developing a new snow model need not understand, modify, or even possess the source code of the framework that will host it. They implement a defined interface---``here is how to preprocess my inputs, here is how to execute me, here is how to read my outputs''---and register. The framework discovers the new model at runtime and makes it available alongside all existing models, with identical preprocessing, calibration, and evaluation infrastructure.

This is not merely a software convenience. It is a \textit{governance model} for decentralized scientific contribution. The interface definition is a social contract: it specifies what the community expects of a model integration and what it provides in return.

\subsection{Declarative Specification as Reproducibility}

A second architectural principle with transformative potential is declarative specification: the practice of describing \textit{what} an experiment should do rather than \textit{how} to do it. In current hydrological practice, experimental designs are encoded in imperative scripts---sequential instructions that acquire data, run models, and produce outputs. These scripts conflate scientific intent with computational implementation, making it difficult to distinguish deliberate choices from implementation artifacts, to compare experiments that differ in only one dimension, or to reproduce results on a different platform.

A declarative configuration separates intent from execution. A researcher specifies: ``I want to simulate the Bow River at Banff using SUMMA with ERA5 forcing, calibrated against streamflow using DDS for 1,000 iterations, evaluated over 2011--2015.'' The framework translates this specification into the required sequence of computational steps. The specification is human-readable, machine-executable, version-controllable, and---critically---diffable: two experiments can be compared by comparing their configuration files, with differences in scientific choices immediately visible.

To make this contrast concrete, consider how the same scientific question---``How does SUMMA perform on the Bow River at Banff when calibrated against streamflow?''---is expressed under each paradigm:

\textbf{(a) Imperative (current practice):}
\begin{verbatim}
# Download ERA5 data (hope the CDS API key hasn't expired)
python download_era5.py --lat 51.17 --lon -115.57 --start 2004 --end 2010
# Remap to catchment (where did I put the weight file?)
python remap_forcing.py --weights weights_bow.nc --input era5_raw/ ...
# Convert units (was it kg/m2/s or mm/hr?)
python convert_units.py --config unit_config.json --input era5_remapped/
# Generate SUMMA input files (which version of the script?)
python setup_summa.py --catchment bow_banff --forcing era5_converted/ ...
# Compile SUMMA (pray)
cd /path/to/summa && make -f Makefile.local
# Run calibration (custom DDS wrapper, 3 months old, undocumented)
python my_dds_calibration.py --model summa --iterations 1000 --obj kge ...
# Evaluate (different script, different author, different conventions)
python evaluate_model.py --sim output/streamflow.nc --obs obs/wsc.csv ...
\end{verbatim}

\textbf{(b) Declarative:}
\begin{verbatim}
experiment:
  name: Bow_at_Banff_SUMMA
  start: "2004-01-01"
  end: "2010-12-31"
domain:
  pour_point: [51.17, -115.57]
forcing:
  dataset: ERA5
model:
  name: SUMMA
optimization:
  algorithm: DDS
  iterations: 1000
  objective: KGE
  calibration_period: ["2004-01-01", "2007-12-31"]
  evaluation_period: ["2008-01-01", "2010-12-31"]
evaluation:
  targets: [STREAMFLOW]
\end{verbatim}

The imperative version encodes the same scientific intent across seven scripts, three data formats, and an unknowable number of implicit assumptions. The declarative version encodes it in 17 lines. The imperative version is a diary. The declarative version is a protocol.

\subsection{Separation of Concerns as a Scaling Strategy}

The most fundamental lesson from software engineering is that complex systems become manageable only when their components are decoupled---when changes to one component do not cascade through the system. In hydrological modeling, the lack of separation between workflow stages is the primary source of technical fragility. A change in forcing dataset should not require modifications to the calibration code. A new model should not require changes to the evaluation pipeline. A switch from laptop to HPC should not require rewriting the preprocessing scripts.

Layered architectures, facade patterns, and well-defined interfaces are the engineering tools for achieving this separation. They are not novel and they are not exciting. But they are the reason that a web developer can build an application without understanding TCP/IP, and that a data scientist can train a neural network without writing CUDA kernels. The accumulated investment in architectural separation is what converts individual expertise into collective capability.

Hydrology has not made this investment. The result is that each researcher must understand every layer of the stack, from file formats to job schedulers, because no layer reliably abstracts the complexity beneath it.

\subsection{Provenance as Infrastructure, Not Afterthought}

Reproducibility in computational science requires more than sharing code \cite{Peng2011, Stodden2016}. It requires capturing the complete computational context: software versions, library dependencies, compiler flags, random seeds, environment variables, and the exact configuration that produced a given result \cite{LeekPeng2015}. In practice, this metadata is almost never recorded systematically because doing so requires effort that produces no immediate scientific return.

The architectural solution is to make provenance capture automatic---a property of the infrastructure rather than a responsibility of the user. When every workflow execution automatically records its environment, configuration, and code version, reproducibility becomes a byproduct of normal operation rather than an additional burden. This is the approach taken by modern MLOps platforms (MLflow, Weights \& Biases), workflow managers (Snakemake, Nextflow), and increasingly by computational biology and climate science.

The hydrological community has recognized the provenance problem \cite{Hutton2016, Gil2016} but has largely addressed it through exhortation---asking researchers to document their workflows more carefully---rather than through infrastructure that makes documentation automatic. Exhortation has not worked. Infrastructure might.

%% ============================================================================
\section{The Missed Convergence}

The architectural principles described in Section 3 are not hypothetical. Over the past decade, the hydrological community has developed, independently and in parallel, many of the components that a unified architecture would require. What is missing is not the pieces but the integration.

\subsection{Three Convergent Traditions}

Three distinct research traditions have advanced toward integration from different starting points, each solving part of the problem while remaining largely unaware of the others' solutions.

\textbf{The workflow tradition} has focused on automating the sequence of steps from raw data to model output. CWARHM \cite{Knoben2022} demonstrated that model-agnostic data preprocessing could be separated from model-specific configuration. eWaterCycle \cite{Hut2022} demonstrated containerized model execution with standardized interfaces. Snakemake \cite{Molder2021} and similar workflow managers brought dependency resolution and resumable execution to scientific pipelines.

\textbf{The multi-hypothesis tradition} has focused on enabling systematic comparison of process representations. SUMMA \cite{Clark2015a, Clark2015b} introduced the idea that a single codebase could support multiple process hypotheses through configurable decision files. FUSE \cite{Clark2008} demonstrated that structural decisions could be systematically varied. MARRMoT \cite{Knoben2019} provided a large library of 46 model structures. Raven \cite{Craig2020} offered a flexible model construction framework. The NextGen framework (NOAA) is pursuing a modular approach using the Basic Model Interface \cite{Peckham2013, Hutton2020}.

\textbf{The data revolution tradition} has focused on the explosion of observational and reanalysis datasets. \citeA{McCabe2017} documented the ``data deluge'' in Earth system science. Satellite missions (GRACE, SMAP, Sentinel, MODIS) and reanalysis products (ERA5, MERRA-2) have provided unprecedented observational constraints. Large-sample datasets like CAMELS \cite{Newman2015, Addor2017} have enabled systematic cross-catchment analysis.

Table~\ref{tab:systems} maps these systems against five design principles. The pattern is telling: no single system addresses all five principles, and the principles most consistently absent---end-to-end orchestration and automatic provenance---are precisely those that require architectural integration across traditions.

\begin{table}[ht]
\centering
\caption{Existing systems assessed against five architectural principles. Symbols: $\bullet$ = core design feature; $\circ$ = partially addressed; -- = not addressed.}
\label{tab:systems}
\begin{tabular}{lccccc}
\toprule
System & Declarative & Registry & End-to-End & Provenance & Portable \\
\midrule
CWARHM & $\circ$ & -- & $\circ$ & -- & $\circ$ \\
eWaterCycle & $\circ$ & $\bullet$ & $\circ$ & $\circ$ & $\bullet$ \\
MARRMoT & -- & $\circ$ & -- & -- & $\bullet$ \\
Raven & $\bullet$ & $\circ$ & $\circ$ & -- & $\circ$ \\
SUMMA & $\bullet$ & -- & -- & -- & $\circ$ \\
FUSE & $\bullet$ & -- & -- & -- & $\circ$ \\
NextGen & $\circ$ & $\bullet$ & $\circ$ & $\circ$ & $\circ$ \\
BMI & -- & $\bullet$ & -- & -- & $\bullet$ \\
Snakemake & $\bullet$ & $\circ$ & $\bullet$ & $\circ$ & $\bullet$ \\
SYMFLUENCE & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Why Convergence Hasn't Happened}

These three traditions address complementary aspects of the same problem. A complete solution requires all three. Yet they have largely developed in isolation, each building solutions that assume different interfaces, directory structures, and execution models.

The reasons are structural rather than intellectual. Different funding agencies support different traditions. Different journals reward different contributions. Different research groups have different technical stacks. And critically, \textit{integration is expensive and unrewarded}. Connecting a workflow system to a multi-hypothesis model to a satellite data pipeline requires deep understanding of all three systems---exactly the impossible-generalist problem identified in Section 2.

\subsection{The Blueprint We Already Had}

It is worth noting that the vision of integrated hydrological modeling is not new. \citeA{FreezeHarlan1969}, in what remains one of the most prescient papers in the field's history, outlined a ``blueprint for a physically-based, digitally-simulated hydrologic response model.'' \citeA{Dooge1986} searched for hydrologic laws that would provide the theoretical foundation for such integration. \citeA{PaniconiPutti2015} reviewed fifty years of progress toward the Freeze-Harlan blueprint and found the computational pieces largely in place.

Fifty-five years later, we have the process understanding, the data, the models, the algorithms, and the hardware that Freeze and Harlan could only imagine. What we still lack is the connective tissue---the shared architectural principles that would allow these components to work together without requiring each researcher to manually forge the connections. The blueprint was drawn. The foundation was never poured.

%% ============================================================================
\section{Principles for a Shared Architecture}

We do not prescribe a specific implementation. Architectural monopolies are as harmful as architectural anarchy. What we advocate is a set of shared principles that, if adopted across the community's diverse tools and frameworks, would enable interoperability, reduce duplicated effort, and free researchers to focus on science rather than plumbing.

\subsection{Interface over Implementation}

The most impactful change the community could adopt is the consistent use of defined interfaces between workflow stages. A model that exposes a standard preprocessing interface can receive inputs from any compliant data pipeline. An evaluation module that accepts a standard output format can assess any compliant model. The interface is the social contract; the implementation is a private matter.

This does not require that the community agree on a single model, a single language, or a single framework. It requires only that the community agree on what information crosses boundaries between components. The Basic Model Interface (BMI) \cite{Peckham2013, Hutton2020} represents an important step in this direction. Extending similar interface thinking to data preprocessing, calibration, and evaluation would complete the picture.

\subsection{Registration over Modification}

Adding a new component to a shared ecosystem should not require modifying existing code. The registry pattern---in which new models, data sources, or algorithms register themselves at import time---provides a mechanism for decentralized contribution that scales with community size rather than with central maintainer bandwidth.

\subsection{Declaration over Instruction}

Experimental designs should be expressed as configurations, not as scripts. A YAML or TOML file specifying ``SUMMA, ERA5, Bow River, DDS, KGE, 2004--2010'' should be sufficient to reproduce an experiment on any compliant platform. This configuration should be the primary artifact of record---versioned, diffable, and publishable alongside results.

\subsection{Provenance by Default}

Every workflow execution should automatically record its computational context: framework version, dependency versions, Git commit hash, resolved configuration, platform details, and execution timestamps. This metadata should be written alongside model outputs without user intervention, creating self-documenting result packages.

\subsection{Separation of Scientific and Computational Concerns}

The most consequential principle is that a researcher's scientific choices should be expressible independently of their computational implementation. The decision to compare three models against GRACE observations is a scientific choice. The decision to run those comparisons in parallel on a SLURM cluster is a computational choice. These decisions should inhabit different configuration sections, be modifiable independently, and never require changes to each other.

This separation is what ultimately breaks the impossible-generalist problem. A process hydrologist should be able to specify a scientific experiment without understanding MPI. An HPC specialist should be able to optimize execution without understanding snow physics. A data scientist should be able to add a new satellite product without understanding model internals. Shared architecture makes specialization productive rather than isolating.

%% ============================================================================
\section{The Path Forward}

\subsection{What We Are Not Arguing}

We are not arguing that any single framework should monopolize hydrological modeling. Monocultures are fragile and stifle innovation. The field benefits from diversity in models, methods, and approaches.

We are not arguing that technical infrastructure is more important than scientific understanding. Process knowledge remains the foundation of credible prediction. Infrastructure is the means by which that knowledge is deployed, tested, and refined.

We are not criticizing the researchers who have built the tools the community relies on. The existing ecosystem represents decades of dedicated, often underrecognized, occasionally heroic work. The models work. The data products work. The algorithms work. The problem is not that any of these tools are inadequate but that the connections between them have been left to individual improvisation rather than community design. We have excellent instruments and no orchestra.

And we are not arguing that the problem is easy. If it were easy, it would have been solved already---and someone would have written a Python package for it, with a name that is either an acronym or a mythological reference, and a logo involving water.

\subsection{What We Are Arguing}

We are arguing that the community's current approach---in which each research group independently builds and maintains the connective infrastructure between workflow stages---is unsustainable, inequitable, and scientifically costly. It is unsustainable because the technical breadth required grows faster than any individual can learn. It is inequitable because it privileges well-resourced groups with dedicated technical support over smaller groups with equal scientific insight. And it is scientifically costly because it prevents the systematic, reproducible experiments needed to resolve fundamental questions about the value of process complexity, the role of forcing uncertainty, and the limits of predictability.

We are arguing that the solution is architectural: a shared set of interface definitions, design patterns, and infrastructure conventions that allow independently developed components to interoperate. This is not a call for a single framework but for a shared language of integration---the equivalent of agreeing on SI units, not on which thermometer to use.

We are arguing that this investment, while initially unrewarded by current incentive structures, would yield compounding returns. Every preprocessing pipeline built to a common interface becomes reusable. Every model integrated through a standard protocol becomes comparable. Every evaluation module built against a defined output format becomes universal. The first group to adopt these principles bears the cost; every subsequent group reaps the benefit.

\subsection{An Invitation}

The principles outlined in this paper are implemented, to varying degrees, in several existing systems. SYMFLUENCE \cite{Eythorsson2025a, Eythorsson2025b} demonstrates that a registry-based, declarative, end-to-end architecture can integrate models spanning Fortran, C, Python, R, and Julia within a single reproducible workflow. eWaterCycle \cite{Hut2022} demonstrates containerized model interoperability. BMI \cite{Hutton2020} demonstrates standardized model interfaces. NextGen demonstrates modular model composition. Each embodies some of the principles advocated here.

What none of them can accomplish alone is the community-wide convergence that would make these principles universal rather than framework-specific. That convergence requires not a technical breakthrough but a collective decision: that the scaffolding matters, that maintaining it is a shared responsibility, and that the researchers who build and maintain it deserve the same recognition as those who build the models that stand upon it.

The hydrologists of the next generation should not need to be outstanding in every field. They should need to be outstanding in \textit{their} field---and to have infrastructure that lets that be enough. The pun, regrettably, writes itself: we need fewer researchers outstanding in every field and more who are allowed to come inside and do science.

%% ============================================================================
\section{Conclusion}

The explosive growth of data, models, algorithms, and computational power in hydrology over the past two decades represents a remarkable collective achievement. That this growth has not translated proportionally into predictive improvement is not an indictment of the science but a symptom of the missing layer between components and systems: the architectural infrastructure that converts individual advances into collective capability.

We have argued that the impossible breadth of expertise now demanded of computational hydrologists is neither necessary nor sustainable---that it reflects an infrastructure deficit, not a talent deficit. The principles needed to address this deficit are well established in software engineering: defined interfaces, registry-based extensibility, declarative specification, automatic provenance, and separation of concerns. These are not speculative ideas but proven practices that have transformed other computational disciplines.

The path forward requires the community to treat workflow architecture as a first-class research contribution---to fund it, publish it, reward it, and maintain it with the same rigor applied to models and datasets. It requires a willingness to converge on shared interface definitions, even when existing tools must be adapted to comply. And it requires recognizing that the researchers who pour the foundation deserve as much credit as those who build the tower.

\citeA{FreezeHarlan1969} imagined a future in which computational power would unlock the full potential of physically-based hydrological modeling. That computational power arrived long ago. What remains is to build the architecture that connects it. The blueprint was drawn fifty-five years ago. It is time to pour the foundation.

%% ============================================================================
%% OPEN RESEARCH STATEMENT (required)
%% ============================================================================
\section*{Open Research}
No new data or code were produced for this commentary. The architectural principles discussed are implemented in the SYMFLUENCE framework, available at [repository URL to be added]. The companion papers \cite{Eythorsson2025a, Eythorsson2025b} provide technical documentation and validation experiments.

%% ============================================================================
%% ACKNOWLEDGMENTS
%% ============================================================================
\acknowledgments
[Acknowledgments to be added.]

%% ============================================================================
%% REFERENCES
%% ============================================================================
\bibliography{references}

\end{document}
