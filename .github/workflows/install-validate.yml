name: SYMFLUENCE - Full Install & Validate

on:
  push:
    branches: [ main, develop ]  # Run on main and develop pushes
  pull_request:
    branches: [ main, develop ]  # Run on PRs to main/develop
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: false
        default: 'quick'
        type: choice
        options:
          - smoke
          - quick
          - full
  schedule:
    - cron: '0 2 * * 0'  # Run weekly on Sundays at 2 AM UTC

defaults:
  run:
    shell: bash

concurrency:
  group: symfluence-${{ github.head_ref || github.ref_name }}
  cancel-in-progress: true

jobs:
  install-validate:
    runs-on: ubuntu-22.04
    timeout-minutes: 90

    env:
      TZ: UTC
      CI: "1"
      NONINTERACTIVE: "1"
      SYMFLUENCE_CODE: ${{ github.workspace }}
      SYMFLUENCE_DATA: ${{ github.workspace }}/symfluence_data
      FC: gfortran

    steps:
      - name: Free up disk space
        run: |
          # Remove unnecessary pre-installed software to free ~20GB
          echo "Disk space before cleanup:"
          df -h
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf /usr/local/share/boost
          sudo rm -rf "$AGENT_TOOLSDIRECTORY"
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/hostedtoolcache/CodeQL
          sudo rm -rf /usr/local/.ghcup
          sudo rm -rf /usr/share/swift
          sudo docker image prune --all --force || true
          sudo apt-get clean
          echo "Disk space after cleanup:"
          df -h

      - name: Checkout
        uses: actions/checkout@v4
        with:
          submodules: recursive
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      # ----- System deps (always run now) -----
      - name: Install system build deps
        run: |
          set -e
          # Add the UbuntuGIS PPA to get a stable, compatible suite of geospatial libraries
          sudo apt-get update
          sudo apt-get install -y software-properties-common
          sudo add-apt-repository -y ppa:ubuntugis/ppa
          sudo apt-get update
          
          # Now install all dependencies.
          # The GDAL and other related packages will now be pulled from the stable PPA.
          sudo apt-get install -y \
            build-essential gfortran cmake make pkg-config \
            ninja-build autoconf automake libtool \
            git wget curl unzip tar \
            openmpi-bin libopenmpi-dev \
            gdal-bin libgdal-dev libproj-dev \
            libnetcdf-dev libnetcdff-dev \
            libhdf5-dev hdf5-tools libudunits2-dev \
            r-base libcurl4-openssl-dev \
            python3-dev python3-pip python3-numpy python3-gdal cdo
          mpicc --version
          mpirun --version
          gdal-config --version

      - name: Configure compiler and library env
        run: |
          set -e
          echo "PKG_CONFIG_PATH=/usr/lib/x86_64-linux-gnu/pkgconfig:/usr/lib/pkgconfig:/usr/share/pkgconfig:${PKG_CONFIG_PATH}" >> $GITHUB_ENV
          NETCDF_PREFIX=$(nc-config --prefix 2>/dev/null || echo /usr)
          NETCDF_LIBDIR=$(nc-config --libdir 2>/dev/null || echo /usr/lib/x86_64-linux-gnu)
          NETCDF_INC=$(nc-config --includedir 2>/dev/null || echo /usr/include)
          echo "NETCDF=${NETCDF_PREFIX}" >> $GITHUB_ENV
          echo "NETCDF_FORTRAN=$(nf-config --prefix 2>/dev/null || echo ${NETCDF_PREFIX})" >> $GITHUB_ENV
          echo "NETCDF_DIR=${NETCDF_PREFIX}" >> $GITHUB_ENV
          echo "NETCDF_LIBDIR=${NETCDF_LIBDIR}" >> $GITHUB_ENV
          echo "NETCDF_INCLUDE=${NETCDF_INC}" >> $GITHUB_ENV
          echo "HDF5_ROOT=/usr/lib/x86_64-linux-gnu/hdf5/serial" >> $GITHUB_ENV
          echo "HDF5_LIBDIR=/usr/lib/x86_64-linux-gnu/hdf5/serial" >> $GITHUB_ENV
          echo "HDF5_INCLUDE_DIR=/usr/include/hdf5/serial" >> $GITHUB_ENV
          echo "UDUNITS2_DIR=$(pkg-config --variable=prefix udunits2 2>/dev/null || echo /usr)" >> $GITHUB_ENV
          echo "LD_LIBRARY_PATH=${NETCDF_LIBDIR}:${HDF5_LIBDIR}:${LD_LIBRARY_PATH}" >> $GITHUB_ENV
          echo "LIBRARY_PATH=${NETCDF_LIBDIR}:${HDF5_LIBDIR}:${LIBRARY_PATH}" >> $GITHUB_ENV
          echo "CC=mpicc"  >> $GITHUB_ENV
          echo "CXX=mpicxx" >> $GITHUB_ENV
          NF_FLIBS="$(nf-config --flibs 2>/dev/null || echo "-L${NETCDF_LIBDIR} -lnetcdff")"
          NC_LIBS="$(nc-config --libs  2>/dev/null || echo "-L${NETCDF_LIBDIR} -lnetcdf")"
          H5_LIBS="$(pkg-config --libs hdf5_hl hdf5 2>/dev/null || echo "-L${HDF5_LIBDIR} -lhdf5_hl -lhdf5")"
          echo "FUSE_LIBRARIES=${NF_FLIBS} ${NC_LIBS} ${H5_LIBS}" >> $GITHUB_ENV
          NF_FFLAGS="$(nf-config --fflags 2>/dev/null || true)"
          NC_CFLAGS="$(nc-config --cflags 2>/dev/null || true)"
          H5_CFLAGS="$(pkg-config --cflags hdf5 2>/dev/null || echo "-I${HDF5_INCLUDE_DIR}")"
          echo "FUSE_INCLUDE=${NC_CFLAGS} ${NF_FFLAGS} ${H5_CFLAGS} -I${NETCDF_INCLUDE}" >> $GITHUB_ENV
          echo "FUSE_FFLAGS_EXTRA=-fallow-argument-mismatch -std=legacy -ffree-line-length-none -fmax-errors=0" >> $GITHUB_ENV

      - name: Create /usr/lib64 symlinks for legacy builds (HDF5/NetCDF)
        run: |
          set -e
          sudo mkdir -p /usr/lib64
          for so in /usr/lib/x86_64-linux-gnu/libnetcdf.so* /usr/lib/x86_64-linux-gnu/libnetcdff.so*; do
            [ -e "$so" ] || continue; base=$(basename "$so")
            [ -e "/usr/lib64/${base}" ] || sudo ln -s "$so" "/usr/lib64/${base}" || true
          done
          for so in /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5.so* \
                    /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5_hl.so* \
                    /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5_fortran.so* \
                    /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5hl_fortran.so*; do
            [ -e "$so" ] || continue; base=$(basename "$so")
            [ -e "/usr/lib64/${base}" ] || sudo ln -s "$so" "/usr/lib64/${base}" || true
          done

      - name: Ensure data dir exists
        run: mkdir -p "${{ env.SYMFLUENCE_DATA }}"

      - name: Cache test data (v0.6.0)
        uses: actions/cache@v4
        with:
          path: ${{ env.SYMFLUENCE_DATA }}
          key: symfluence-testdata-v0.6.0-${{ hashFiles('tests/pytest.ini', 'tests/fixtures/data_fixtures.py') }}
          restore-keys: |
            symfluence-testdata-v0.6.0-

      - name: Full install (build venv + tools)
        env:
          CC: mpicc
          CXX: mpicxx
          FC: gfortran
        run: |
          set -e
          chmod +x ./symfluence
          ./symfluence --install
          
          # Install CPU-only torch to save space (Linux only)
          source venv/bin/activate
          pip install torch --index-url https://download.pytorch.org/whl/cpu --force-reinstall


      - name: Add external tools to PATH
        run: |
          set -e
          # Add SUMMA
          echo "${{ env.SYMFLUENCE_DATA }}/installs/summa/bin" >> $GITHUB_PATH
          # Add mizuRoute
          echo "${{ env.SYMFLUENCE_DATA }}/installs/mizuRoute/route/bin" >> $GITHUB_PATH
          # Add TauDEM
          echo "${{ env.SYMFLUENCE_DATA }}/installs/TauDEM/bin" >> $GITHUB_PATH

          # Add SUNDIALS library path for SUMMA runtime dependencies
          SUNDIALS_LIB="${{ env.SYMFLUENCE_DATA }}/installs/sundials/install/sundials/lib"
          if [ -d "$SUNDIALS_LIB" ]; then
            echo "LD_LIBRARY_PATH=$SUNDIALS_LIB:/usr/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}" >> $GITHUB_ENV
            echo "✓ Added SUNDIALS library path: $SUNDIALS_LIB"
          else
            echo "⚠ SUNDIALS library directory not found at: $SUNDIALS_LIB"
            ls -la "${{ env.SYMFLUENCE_DATA }}/installs/sundials/" || true
          fi

      - name: Activate venv for subsequent steps
        run: |
          set -e
          echo "VIRTUAL_ENV=${{ env.SYMFLUENCE_CODE }}/venv" >> $GITHUB_ENV
          echo "${{ env.SYMFLUENCE_CODE }}/venv/bin" >> $GITHUB_PATH

      - name: Debug SUMMA library dependencies
        run: |
          echo "========================================="
          echo "Debugging SUMMA library dependencies"
          echo "========================================="
          echo "LD_LIBRARY_PATH: ${LD_LIBRARY_PATH:-not set}"
          echo ""
          echo "SUMMA binary location:"
          which summa_sundials.exe || echo "SUMMA not in PATH"
          echo ""
          echo "Running ldd on SUMMA binary:"
          ldd ${{ env.SYMFLUENCE_DATA }}/installs/summa/bin/summa_sundials.exe || echo "ldd failed"
          echo ""
          echo "Checking SUNDIALS libraries:"
          ls -la ${{ env.SYMFLUENCE_DATA }}/installs/sundials/install/sundials/lib/ || echo "SUNDIALS lib directory not found"
          echo "========================================="

      - name: Determine test level
        id: test_level
        run: |
          # Determine which test level to run
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            TEST_LEVEL="${{ github.event.inputs.test_level }}"
          elif [ "${{ github.event_name }}" = "schedule" ]; then
            TEST_LEVEL="full"
          elif [ "${{ github.ref }}" = "refs/heads/main" ]; then
            TEST_LEVEL="full"
          else
            TEST_LEVEL="quick"
          fi
          echo "test_level=${TEST_LEVEL}" >> $GITHUB_OUTPUT
          echo "Running test level: ${TEST_LEVEL}"

      - name: Run validation tests
        env:
          PYTHONPATH: ${{ env.SYMFLUENCE_CODE }}:${{ env.SYMFLUENCE_CODE }}/tests
        run: |
          set -e

          TEST_LEVEL="${{ steps.test_level.outputs.test_level }}"
          echo "========================================="
          echo "Running ${TEST_LEVEL} test suite"
          echo "========================================="

          case $TEST_LEVEL in
            smoke)
              echo "SMOKE TESTS: Minimal validation (~5 min)"
              echo "- Binary validation (SUMMA, mizuRoute, TauDEM)"
              echo "- Package imports"
              echo "- 3-hour SUMMA workflow"
              echo ""
              pytest -v --tb=short -m "smoke" tests/
              ;;
            quick)
              echo "QUICK TESTS: Comprehensive validation (~20 min)"
              echo "1. Unit tests (all)"
              echo "2. Binary validation"
              echo "3. Package imports"
              echo "4. Integration tests (basic)"
              echo "5. Quick workflow (3-hour SUMMA)"
              echo ""

              # Run unit tests first (fast)
              echo "========================================="
              echo "Running unit tests..."
              echo "========================================="
              pytest -v --tb=short -m "unit" tests/unit/ || true

              # Run all ci_quick tests (includes smoke, binary validation, quick workflows)
              echo "========================================="
              echo "Running quick integration & e2e tests..."
              echo "========================================="
              pytest -v --tb=short -m "ci_quick" tests/
              ;;
            full)
              echo "FULL TESTS: Complete validation suite (~90 min)"
              echo "1. Unit tests (all)"
              echo "2. Integration tests (all)"
              echo "3. Binary validation"
              echo "4. Package imports"
              echo "5. Quick workflow (3-hour SUMMA)"
              echo "6. Full workflow (1-month SUMMA + mizuRoute)"
              echo "7. Calibration workflow"
              echo ""

              # Run unit tests
              echo "========================================="
              echo "1/4: Running unit tests..."
              echo "========================================="
              pytest -v --tb=short -m "unit" tests/unit/ || true

              # Run integration tests
              echo "========================================="
              echo "2/4: Running integration tests..."
              echo "========================================="
              pytest -v --tb=short -m "integration" tests/integration/ || true

              # Run quick e2e tests
              echo "========================================="
              echo "3/4: Running quick e2e tests..."
              echo "========================================="
              pytest -v --tb=short -m "ci_quick" tests/e2e/

              # Run full e2e tests (long-running)
              echo "========================================="
              echo "4/4: Running full e2e tests (1-month simulations)..."
              echo "========================================="
              pytest -v --tb=short -m "ci_full" tests/e2e/
              ;;
            *)
              echo "Unknown test level: ${TEST_LEVEL}"
              exit 1
              ;;
          esac

          echo ""
          echo "========================================="
          echo "✓ All ${TEST_LEVEL} tests completed"
          echo "========================================="

      # =====================================================================
      # PHASE 1: Generate Release Artifacts
      # =====================================================================
      - name: Generate toolchain metadata
        if: success()
        run: |
          set -e
          chmod +x ./scripts/generate_toolchain.sh
          ./scripts/generate_toolchain.sh \
            "${{ env.SYMFLUENCE_DATA }}/installs" \
            "${{ env.SYMFLUENCE_DATA }}/installs/toolchain.json" \
            "linux-x86_64"

      - name: Stage release artifacts
        if: success()
        run: |
          set -e
          chmod +x ./scripts/stage_release_artifacts.sh
          mkdir -p ./release
          ./scripts/stage_release_artifacts.sh \
            "linux-x86_64" \
            "${{ env.SYMFLUENCE_DATA }}/installs" \
            "./release"

      - name: Verify binary portability
        if: success()
        run: |
          set -e
          chmod +x ./scripts/verify_binary_portability.sh
          ./scripts/verify_binary_portability.sh \
            "./release/symfluence-tools/bin" || {
            echo "⚠️  Portability verification had warnings but continuing..."
            echo "   Review the output above for potential issues"
          }

      - name: Create release tarball
        if: success()
        run: |
          set -e
          chmod +x ./scripts/create_release_tarball.sh
          VERSION="${{ github.ref_name }}"
          ./scripts/create_release_tarball.sh \
            "$VERSION" \
            "linux-x86_64" \
            "./release/symfluence-tools" \
            "./release"

      - name: Test artifact relocatability
        if: success()
        run: |
          set -e
          echo "Testing relocated binaries..."

          # Extract to temporary location
          mkdir -p /tmp/symfluence-test
          cd /tmp/symfluence-test
          tar -xzf $GITHUB_WORKSPACE/release/symfluence-tools-*.tar.gz

          # Test binaries run without original install directory
          echo "→ Testing SUMMA..."
          if [ -f "./symfluence-tools/bin/summa" ]; then
            ./symfluence-tools/bin/summa --version 2>&1 | head -5 || echo "  SUMMA version check completed"
          fi

          echo "→ Testing NGEN..."
          if [ -f "./symfluence-tools/bin/ngen" ]; then
            ./symfluence-tools/bin/ngen --help 2>&1 | head -5 || echo "  NGEN help check completed"
          fi

          echo "→ Testing mizuRoute..."
          if [ -f "./symfluence-tools/bin/mizuroute" ]; then
            # mizuRoute doesn't have --version, just check it exists and is executable
            [ -x "./symfluence-tools/bin/mizuroute" ] && echo "  mizuRoute is executable"
          fi

          echo "✓ Relocatability test complete"

      - name: Upload release artifacts
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: symfluence-tools-linux-x86_64
          path: |
            release/symfluence-tools-*.tar.gz
            release/symfluence-tools-*.sha256
          retention-days: 30

      - name: Upload test outputs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: test-outputs
          path: |
            ${{ env.SYMFLUENCE_DATA }}/example_data_v0.6.0/domain_*/simulations/
            ${{ env.SYMFLUENCE_DATA }}/example_data_v0.6.0/domain_*/settings/
            ${{ env.SYMFLUENCE_DATA }}/example_data_v0.6.0/domain_*/forcing/
          if-no-files-found: ignore

      - name: Upload pytest logs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: pytest-logs
          path: |
            pytest*.log
            .pytest_cache/
          if-no-files-found: ignore
      - name: Validate workflow status API
        if: success()
        run: |
          set -e
          . ./venv/bin/activate || . ./.venv/bin/activate || true
          python - << 'PY'
          from pathlib import Path
          from symfluence import SYMFLUENCE
          c = SYMFLUENCE(Path('./0_config_files/config_template.yaml'))
          s = c.get_workflow_status()
          assert isinstance(s, dict) and {'total_steps','step_details'}.issubset(s), s
          print("API status OK:", s['total_steps'], "steps")
          PY

  step_tests:
    needs: install-validate
    runs-on: ubuntu-22.04
    env:
      TZ: UTC
      CI: "1"
      NONINTERACTIVE: "1"
      SYMFLUENCE_CODE: ${{ github.workspace }}
      # Put CI working data outside the repo tree
      SYMFLUENCE_DATA: ${{ github.workspace }}/.sym_data
      DOMAIN: Bow_at_Banff
      DOMAIN_DEF: delineate
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          submodules: recursive
          fetch-depth: 0

      # Reuse your existing install procedure; keep if not inherited from prior job
      - name: Install SYMFLUENCE (wrapper)
        run: |
          set -e
          chmod +x ./symfluence
          ./symfluence --install

      - name: Create CI config for Bow at Banff
        run: |
          set -e
          mkdir -p 0_config_files "${SYMFLUENCE_DATA}"
          cat > 0_config_files/config_ci_bow.yaml <<'YAML'
          SYMFLUENCE_CODE_DIR: "${{ env.SYMFLUENCE_CODE }}"
          SYMFLUENCE_DATA_DIR: "${{ env.SYMFLUENCE_DATA }}"
          DOMAIN_NAME: "${{ env.DOMAIN }}"
          DOMAIN_DEFINITION_METHOD: "${{ env.DOMAIN_DEF }}"
          LOG_LEVEL: "INFO"
          YAML

      - name: Step 1 — setup_project (creates domain scaffolding)
        run: |
          set -e
          ./symfluence workflow step setup_project --config 0_config_files/config_ci_bow.yaml

      - name: Step 2 — copy repo data into created domain folder
        run: |
          set -e
          SRC="./src/data/domain_${DOMAIN}"
          DEST="${SYMFLUENCE_DATA}/domain_${DOMAIN}"
          mkdir -p "${DEST}"
          # Copy everything (shapefiles, any sidecars, etc.)
          cp -a "${SRC}/." "${DEST}/"
          echo "Copied data into ${DEST}"
          ls -R "${DEST}" || true

      - name: Step 3 — run define_domain using CLI
        run: |
          set -e
          ./symfluence workflow step define_domain --config 0_config_files/config_ci_bow.yaml

      - name: Assertions — river basins shapefile exists; logs exist
        run: |
          set -e
          DOMAIN_DIR="${SYMFLUENCE_DATA}/domain_${DOMAIN}"
          RIV_DIR="${DOMAIN_DIR}/shapefiles/river_basins"
          # Expect a river basins shapefile (method suffix may vary; allow wildcard)
          test -d "${RIV_DIR}"
          ls "${RIV_DIR}/${DOMAIN}_riverBasins_"*.shp >/dev/null
          # Also check that workflow logs are written in the standard location
          ls "_workLog_${DOMAIN}" >/dev/null
          echo "✅ Expected outputs present."

      - name: Upload artifacts on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: domain-define-minimal-logs
          path: |
            _workLog_${{ env.DOMAIN }}/**
            ${{ env.SYMFLUENCE_DATA }}/domain_${{ env.DOMAIN }}/**/*
          if-no-files-found: warn
