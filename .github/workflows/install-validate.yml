name: SYMFLUENCE - Full Install & Validate

on:
  push:
    branches: [ main, develop ]  # Run on main and develop pushes
  pull_request:
    branches: [ main, develop ]  # Run on PRs to main/develop
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: false
        default: 'quick'
        type: choice
        options:
          - smoke
          - quick
          - full
  schedule:
    - cron: '0 2 * * 0'  # Run weekly on Sundays at 2 AM UTC

defaults:
  run:
    shell: bash

concurrency:
  group: symfluence-${{ github.head_ref || github.ref_name }}
  cancel-in-progress: true

jobs:
  install-validate:
    runs-on: ubuntu-24.04
    timeout-minutes: 90

    env:
      TZ: UTC
      CI: "1"
      NONINTERACTIVE: "1"
      SYMFLUENCE_CODE: ${{ github.workspace }}
      SYMFLUENCE_DATA: ${{ github.workspace }}/symfluence_data
      FC: gfortran

    steps:
      - name: Free up disk space
        run: |
          # Remove unnecessary pre-installed software to free ~20GB
          echo "Disk space before cleanup:"
          df -h
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf /usr/local/share/boost
          sudo rm -rf "$AGENT_TOOLSDIRECTORY"
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/hostedtoolcache/CodeQL
          sudo rm -rf /usr/local/.ghcup
          sudo rm -rf /usr/share/swift
          sudo docker image prune --all --force || true
          sudo apt-get clean
          echo "Disk space after cleanup:"
          df -h

      - name: Checkout
        uses: actions/checkout@v4
        with:
          submodules: recursive
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      # ----- System deps (always run now) -----
      - name: Install system build deps
        run: |
          set -e
          sudo apt-get update
          sudo apt-get install -y \
            build-essential gfortran cmake make pkg-config \
            ninja-build autoconf automake libtool \
            git wget curl unzip tar \
            openmpi-bin libopenmpi-dev \
            gdal-bin libgdal-dev libproj-dev \
            libnetcdf-dev libnetcdff-dev \
            libhdf5-dev hdf5-tools libudunits2-dev \
            libfl-dev libbsd-dev \
            r-base libcurl4-openssl-dev \
            python3-dev python3-pip python3-numpy cdo
          mpicc --version
          mpirun --version
          gdal-config --version

      - name: Configure compiler and library env
        run: |
          set -e
          echo "PKG_CONFIG_PATH=/usr/lib/x86_64-linux-gnu/pkgconfig:/usr/lib/pkgconfig:/usr/share/pkgconfig:${PKG_CONFIG_PATH}" >> $GITHUB_ENV
          NETCDF_PREFIX=$(nc-config --prefix 2>/dev/null || echo /usr)
          NETCDF_LIBDIR=$(nc-config --libdir 2>/dev/null || echo /usr/lib/x86_64-linux-gnu)
          NETCDF_INC=$(nc-config --includedir 2>/dev/null || echo /usr/include)
          echo "NETCDF=${NETCDF_PREFIX}" >> $GITHUB_ENV
          echo "NETCDF_FORTRAN=$(nf-config --prefix 2>/dev/null || echo ${NETCDF_PREFIX})" >> $GITHUB_ENV
          echo "NETCDF_DIR=${NETCDF_PREFIX}" >> $GITHUB_ENV
          echo "NETCDF_LIBDIR=${NETCDF_LIBDIR}" >> $GITHUB_ENV
          echo "NETCDF_INCLUDE=${NETCDF_INC}" >> $GITHUB_ENV
          echo "HDF5_ROOT=/usr/lib/x86_64-linux-gnu/hdf5/serial" >> $GITHUB_ENV
          echo "HDF5_LIBDIR=/usr/lib/x86_64-linux-gnu/hdf5/serial" >> $GITHUB_ENV
          echo "HDF5_INCLUDE_DIR=/usr/include/hdf5/serial" >> $GITHUB_ENV
          echo "UDUNITS2_DIR=$(pkg-config --variable=prefix udunits2 2>/dev/null || echo /usr)" >> $GITHUB_ENV
          # LIBRARY_PATH is for compile-time linking (Fortran builds need this)
          echo "LIBRARY_PATH=${NETCDF_LIBDIR}:${HDF5_LIBDIR}:${LIBRARY_PATH}" >> $GITHUB_ENV
          # NOTE: Do NOT set LD_LIBRARY_PATH here — system HDF5 paths on
          # LD_LIBRARY_PATH override pip netCDF4's bundled HDF5 (LD_LIBRARY_PATH
          # beats RUNPATH in the dynamic linker). Fortran builds use -L flags
          # from FUSE_LIBRARIES/pkg-config; runtime needs are handled per-step.
          echo "CC=mpicc"  >> $GITHUB_ENV
          echo "CXX=mpicxx" >> $GITHUB_ENV
          NF_FLIBS="$(nf-config --flibs 2>/dev/null || echo "-L${NETCDF_LIBDIR} -lnetcdff")"
          NC_LIBS="$(nc-config --libs  2>/dev/null || echo "-L${NETCDF_LIBDIR} -lnetcdf")"
          H5_LIBS="$(pkg-config --libs hdf5_hl hdf5 2>/dev/null || echo "-L${HDF5_LIBDIR} -lhdf5_hl -lhdf5")"
          echo "FUSE_LIBRARIES=${NF_FLIBS} ${NC_LIBS} ${H5_LIBS}" >> $GITHUB_ENV
          NF_FFLAGS="$(nf-config --fflags 2>/dev/null || true)"
          NC_CFLAGS="$(nc-config --cflags 2>/dev/null || true)"
          H5_CFLAGS="$(pkg-config --cflags hdf5 2>/dev/null || echo "-I${HDF5_INCLUDE_DIR}")"
          echo "FUSE_INCLUDE=${NC_CFLAGS} ${NF_FFLAGS} ${H5_CFLAGS} -I${NETCDF_INCLUDE}" >> $GITHUB_ENV
          echo "FUSE_FFLAGS_EXTRA=-fallow-argument-mismatch -std=legacy -ffree-line-length-none -fmax-errors=0" >> $GITHUB_ENV

      - name: Create /usr/lib64 symlinks for legacy builds (HDF5/NetCDF)
        run: |
          set -e
          sudo mkdir -p /usr/lib64
          for so in /usr/lib/x86_64-linux-gnu/libnetcdf.so* /usr/lib/x86_64-linux-gnu/libnetcdff.so*; do
            [ -e "$so" ] || continue; base=$(basename "$so")
            [ -e "/usr/lib64/${base}" ] || sudo ln -s "$so" "/usr/lib64/${base}" || true
          done
          for so in /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5.so* \
                    /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5_hl.so* \
                    /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5_fortran.so* \
                    /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5hl_fortran.so*; do
            [ -e "$so" ] || continue; base=$(basename "$so")
            [ -e "/usr/lib64/${base}" ] || sudo ln -s "$so" "/usr/lib64/${base}" || true
          done

      - name: Ensure data dir exists
        run: mkdir -p "${{ env.SYMFLUENCE_DATA }}"

      - name: Cache test data (v0.6.0)
        uses: actions/cache@v4
        with:
          path: ${{ env.SYMFLUENCE_DATA }}
          key: symfluence-testdata-v0.6.0b-${{ hashFiles('tests/pytest.ini', 'tests/fixtures/data_fixtures.py') }}
          restore-keys: |
            symfluence-testdata-v0.6.0b-

      - name: Full install (build venv + tools)
        env:
          CC: mpicc
          CXX: mpicxx
          FC: gfortran
          PYTHONIOENCODING: utf-8
        run: |
          set -e
          chmod +x ./scripts/symfluence-bootstrap
          ./scripts/symfluence-bootstrap --install

          # Install CPU-only torch to save space (Linux only)
          source venv/bin/activate
          pip install torch --index-url https://download.pytorch.org/whl/cpu --force-reinstall

          # Install test dependencies (pytest moved to optional deps)
          pip install -e ".[test]"

          # Rebuild h5py and netCDF4 from source against system HDF5 to prevent
          # dual-loading conflicts — pip wheels each bundle their own libhdf5,
          # and loading two copies in one process causes [Errno -101] HDF errors.
          echo "Rebuilding h5py/netCDF4 against system HDF5..."
          pip install cython numpy  # build deps
          HDF5_PKGCONFIG_NAME=hdf5 pip install --no-binary h5py --force-reinstall h5py || \
            echo "WARNING: h5py source build failed, using wheel"
          pip install --no-binary netCDF4 --force-reinstall netCDF4 || \
            echo "WARNING: netCDF4 source build failed, using wheel"


      - name: Add external tools to PATH
        run: |
          set -e
          # Add SUMMA
          echo "${{ env.SYMFLUENCE_DATA }}/installs/summa/bin" >> $GITHUB_PATH
          # Add mizuRoute
          echo "${{ env.SYMFLUENCE_DATA }}/installs/mizuRoute/route/bin" >> $GITHUB_PATH
          # Add TauDEM
          echo "${{ env.SYMFLUENCE_DATA }}/installs/TauDEM/bin" >> $GITHUB_PATH

          # Add SUNDIALS library path for SUMMA runtime dependencies
          # NOTE: Do NOT add /usr/lib/x86_64-linux-gnu — it's a standard ldconfig path
          # and adding it to LD_LIBRARY_PATH causes system HDF5 1.10 to override
          # pip netCDF4's bundled HDF5 1.14+ (LD_LIBRARY_PATH beats RUNPATH).
          SUNDIALS_LIB="${{ env.SYMFLUENCE_DATA }}/installs/sundials/install/sundials/lib"
          if [ -d "$SUNDIALS_LIB" ]; then
            echo "LD_LIBRARY_PATH=${SUNDIALS_LIB}" >> $GITHUB_ENV
            echo "Added SUNDIALS library path: $SUNDIALS_LIB"
          else
            echo "SUNDIALS library directory not found at: $SUNDIALS_LIB"
            ls -la "${{ env.SYMFLUENCE_DATA }}/installs/sundials/" || true
          fi
          # Disable HDF5 file locking (common source of errors in CI)
          echo "HDF5_USE_FILE_LOCKING=FALSE" >> $GITHUB_ENV

      - name: Activate venv for subsequent steps
        run: |
          set -e
          echo "VIRTUAL_ENV=${{ env.SYMFLUENCE_CODE }}/venv" >> $GITHUB_ENV
          echo "${{ env.SYMFLUENCE_CODE }}/venv/bin" >> $GITHUB_PATH

      - name: Debug SUMMA library dependencies
        run: |
          echo "========================================="
          echo "Debugging SUMMA library dependencies"
          echo "========================================="
          echo "LD_LIBRARY_PATH: ${LD_LIBRARY_PATH:-not set}"
          echo ""
          echo "SUMMA binary location:"
          which summa_sundials.exe || echo "SUMMA not in PATH"
          echo ""
          echo "Running ldd on SUMMA binary:"
          ldd ${{ env.SYMFLUENCE_DATA }}/installs/summa/bin/summa_sundials.exe || echo "ldd failed"
          echo ""
          echo "Checking SUNDIALS libraries:"
          ls -la ${{ env.SYMFLUENCE_DATA }}/installs/sundials/install/sundials/lib/ || echo "SUNDIALS lib directory not found"
          echo "========================================="

      - name: Determine test level
        id: test_level
        run: |
          # Determine which test level to run
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            TEST_LEVEL="${{ github.event.inputs.test_level }}"
          elif [ "${{ github.event_name }}" = "schedule" ]; then
            TEST_LEVEL="full"
          elif [ "${{ github.ref }}" = "refs/heads/main" ]; then
            TEST_LEVEL="full"
          else
            TEST_LEVEL="quick"
          fi
          echo "test_level=${TEST_LEVEL}" >> $GITHUB_OUTPUT
          echo "Running test level: ${TEST_LEVEL}"

      - name: Run validation tests
        # Note: PYTHONPATH removed - editable install handles imports
        run: |
          set -e

          # Unset build-time HDF5 env vars — they can interfere with pip's bundled HDF5
          unset HDF5_ROOT HDF5_LIBDIR HDF5_INCLUDE_DIR

          # Debug: verify which HDF5 libraries Python loads
          echo "LD_LIBRARY_PATH=${LD_LIBRARY_PATH:-<unset>}"
          python -c "
          import h5py, netCDF4
          print(f'h5py {h5py.__version__}, HDF5: {h5py.version.hdf5_version}')
          print(f'netCDF4 {netCDF4.__version__}, HDF5: {netCDF4.__hdf5libversion__}, netCDF: {netCDF4.__netcdf4libversion__}')
          same = h5py.version.hdf5_version == netCDF4.__hdf5libversion__
          print(f'HDF5 versions match: {same}' + (' (good)' if same else ' *** MISMATCH — dual-loading risk ***'))
          " || echo "Could not print HDF5 version info"

          TEST_LEVEL="${{ steps.test_level.outputs.test_level }}"
          echo "========================================="
          echo "Running ${TEST_LEVEL} test suite"
          echo "========================================="

          case $TEST_LEVEL in
            smoke)
              echo "SMOKE TESTS: Minimal validation (~5 min)"
              echo "- Binary validation (SUMMA, mizuRoute, TauDEM)"
              echo "- Package imports"
              echo "- 3-hour SUMMA workflow"
              echo ""
              pytest -v --tb=short -m "smoke" tests/
              ;;
            quick)
              echo "QUICK TESTS: Comprehensive validation (~20 min)"
              echo "1. Unit tests (all, parallel)"
              echo "2. Binary validation"
              echo "3. Package imports"
              echo "4. Integration tests (basic)"
              echo "5. Quick workflow (3-hour SUMMA)"
              echo ""

              # Run unit tests first (parallel for speed)
              echo "========================================="
              echo "Running unit tests (parallel)..."
              echo "========================================="
              pytest --tb=short -m "unit" -q tests/unit/

              # Run all ci_quick tests (includes smoke, binary validation, quick workflows)
              # Serial for e2e tests that need exclusive resource access
              echo "========================================="
              echo "Running quick integration & e2e tests..."
              echo "========================================="
              pytest -v --tb=short -n 0 -m "ci_quick" tests/
              ;;
            full)
              echo "FULL TESTS: Complete validation suite (~90 min)"
              echo "1. Unit tests (all)"
              echo "2. Integration tests (all)"
              echo "3. Binary validation"
              echo "4. Package imports"
              echo "5. Quick workflow (3-hour SUMMA)"
              echo "6. Full workflow (1-month SUMMA + mizuRoute)"
              echo "7. Calibration workflow"
              echo ""

              # Run unit tests (parallel for speed)
              echo "========================================="
              echo "1/4: Running unit tests (parallel)..."
              echo "========================================="
              pytest --tb=short -m "unit" -q tests/unit/

              # Run integration tests (serial - external model binaries)
              echo "========================================="
              echo "2/4: Running integration tests..."
              echo "========================================="
              pytest -v --tb=short -n 0 -m "integration and not calibration and not slow" tests/integration/

              # Run quick e2e tests (serial - workflow tests)
              echo "========================================="
              echo "3/4: Running quick e2e tests..."
              echo "========================================="
              pytest -v --tb=short -n 0 -m "ci_quick" tests/e2e/

              # Run full e2e tests (serial - long-running)
              echo "========================================="
              echo "4/4: Running full e2e tests (1-month simulations)..."
              echo "========================================="
              pytest -v --tb=short -n 0 -m "ci_full" tests/e2e/
              ;;
            *)
              echo "Unknown test level: ${TEST_LEVEL}"
              exit 1
              ;;
          esac

          echo ""
          echo "========================================="
          echo "All ${TEST_LEVEL} tests completed"
          echo "========================================="

      # =====================================================================
      # PHASE 1: Generate Release Artifacts
      # =====================================================================
      - name: Generate toolchain metadata
        if: success()
        run: |
          set -e
          chmod +x ./scripts/generate_toolchain.sh
          ./scripts/generate_toolchain.sh \
            "${{ env.SYMFLUENCE_DATA }}/installs" \
            "${{ env.SYMFLUENCE_DATA }}/installs/toolchain.json" \
            "linux-x86_64"

      - name: Stage release artifacts
        if: success()
        run: |
          set -e
          chmod +x ./scripts/stage_release_artifacts.sh
          mkdir -p ./release
          ./scripts/stage_release_artifacts.sh \
            "linux-x86_64" \
            "${{ env.SYMFLUENCE_DATA }}/installs" \
            "./release"

      - name: Verify binary portability
        if: success()
        run: |
          set -e
          chmod +x ./scripts/verify_binary_portability.sh
          ./scripts/verify_binary_portability.sh \
            "./release/symfluence-tools/bin" || {
            echo "Portability verification had warnings but continuing..."
            echo "   Review the output above for potential issues"
          }

      - name: Create release tarball
        if: success()
        run: |
          set -e
          chmod +x ./scripts/create_release_tarball.sh
          VERSION="${{ github.ref_name }}"
          ./scripts/create_release_tarball.sh \
            "$VERSION" \
            "linux-x86_64" \
            "./release/symfluence-tools" \
            "./release"

      - name: Test artifact relocatability
        if: success()
        run: |
          set -e
          echo "Testing relocated binaries..."

          # Extract to temporary location
          mkdir -p /tmp/symfluence-test
          cd /tmp/symfluence-test
          tar -xzf $GITHUB_WORKSPACE/release/symfluence-tools-*.tar.gz

          # Test binaries run without original install directory
          echo "-> Testing SUMMA..."
          if [ -f "./symfluence-tools/bin/summa" ]; then
            ./symfluence-tools/bin/summa --version 2>&1 | head -5 || echo "  SUMMA version check completed"
          fi

          echo "-> Testing NGEN..."
          if [ -f "./symfluence-tools/bin/ngen" ]; then
            ./symfluence-tools/bin/ngen --help 2>&1 | head -5 || echo "  NGEN help check completed"
          fi

          echo "-> Testing mizuRoute..."
          if [ -f "./symfluence-tools/bin/mizuroute" ]; then
            # mizuRoute doesn't have --version, just check it exists and is executable
            [ -x "./symfluence-tools/bin/mizuroute" ] && echo "  mizuRoute is executable"
          fi

          echo "Relocatability test complete"

      - name: Upload release artifacts
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: symfluence-tools-linux-x86_64
          path: |
            release/symfluence-tools-*.tar.gz
            release/symfluence-tools-*.sha256
          retention-days: 30

      - name: Upload test outputs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: test-outputs
          path: |
            ${{ env.SYMFLUENCE_DATA }}/example_data_v0.6.0/domain_*/simulations/
            ${{ env.SYMFLUENCE_DATA }}/example_data_v0.6.0/domain_*/settings/
            ${{ env.SYMFLUENCE_DATA }}/example_data_v0.6.0/domain_*/forcing/
          if-no-files-found: ignore

      - name: Upload pytest logs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: pytest-logs
          path: |
            pytest*.log
            .pytest_cache/
          if-no-files-found: ignore
      # Note: Workflow status API validation is performed in step_tests job
      # which creates a proper test config file first

  step_tests:
    needs: install-validate
    runs-on: ubuntu-24.04
    env:
      TZ: UTC
      CI: "1"
      NONINTERACTIVE: "1"
      SYMFLUENCE_CODE: ${{ github.workspace }}
      # Put CI working data outside the repo tree
      SYMFLUENCE_DATA: ${{ github.workspace }}/.sym_data
      DOMAIN: Bow_at_Banff
      DOMAIN_DEF: delineate
    steps:
      - name: Free up disk space
        run: |
          # Remove unnecessary pre-installed software to free ~20GB
          echo "Disk space before cleanup:"
          df -h
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf /usr/local/share/boost
          sudo rm -rf "$AGENT_TOOLSDIRECTORY"
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/hostedtoolcache/CodeQL
          sudo rm -rf /usr/local/.ghcup
          sudo rm -rf /usr/share/swift
          sudo docker image prune --all --force || true
          sudo apt-get clean
          echo "Disk space after cleanup:"
          df -h

      - name: Checkout
        uses: actions/checkout@v4
        with:
          submodules: recursive
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install system dependencies
        run: |
          set -e
          sudo apt-get update
          sudo apt-get install -y \
            build-essential gfortran cmake make pkg-config \
            ninja-build autoconf automake libtool \
            git wget curl unzip tar \
            openmpi-bin libopenmpi-dev \
            gdal-bin libgdal-dev libproj-dev \
            libnetcdf-dev libnetcdff-dev \
            libhdf5-dev hdf5-tools libudunits2-dev \
            libfl-dev libbsd-dev \
            r-base libcurl4-openssl-dev \
            python3-dev python3-pip python3-numpy cdo

      - name: Configure compiler and library env
        run: |
          set -e
          echo "PKG_CONFIG_PATH=/usr/lib/x86_64-linux-gnu/pkgconfig:/usr/lib/pkgconfig:/usr/share/pkgconfig:${PKG_CONFIG_PATH}" >> $GITHUB_ENV
          NETCDF_PREFIX=$(nc-config --prefix 2>/dev/null || echo /usr)
          NETCDF_LIBDIR=$(nc-config --libdir 2>/dev/null || echo /usr/lib/x86_64-linux-gnu)
          NETCDF_INC=$(nc-config --includedir 2>/dev/null || echo /usr/include)
          echo "NETCDF=${NETCDF_PREFIX}" >> $GITHUB_ENV
          echo "NETCDF_FORTRAN=$(nf-config --prefix 2>/dev/null || echo ${NETCDF_PREFIX})" >> $GITHUB_ENV
          echo "NETCDF_DIR=${NETCDF_PREFIX}" >> $GITHUB_ENV
          echo "NETCDF_LIBDIR=${NETCDF_LIBDIR}" >> $GITHUB_ENV
          echo "NETCDF_INCLUDE=${NETCDF_INC}" >> $GITHUB_ENV
          echo "HDF5_ROOT=/usr/lib/x86_64-linux-gnu/hdf5/serial" >> $GITHUB_ENV
          echo "HDF5_LIBDIR=/usr/lib/x86_64-linux-gnu/hdf5/serial" >> $GITHUB_ENV
          echo "HDF5_INCLUDE_DIR=/usr/include/hdf5/serial" >> $GITHUB_ENV
          echo "UDUNITS2_DIR=$(pkg-config --variable=prefix udunits2 2>/dev/null || echo /usr)" >> $GITHUB_ENV
          # LIBRARY_PATH is for compile-time linking (Fortran builds need this)
          echo "LIBRARY_PATH=${NETCDF_LIBDIR}:${HDF5_LIBDIR}:${LIBRARY_PATH}" >> $GITHUB_ENV
          # NOTE: Do NOT set LD_LIBRARY_PATH here — system HDF5 paths on
          # LD_LIBRARY_PATH override pip netCDF4's bundled HDF5 (LD_LIBRARY_PATH
          # beats RUNPATH in the dynamic linker). Fortran builds use -L flags
          # from FUSE_LIBRARIES/pkg-config; runtime needs are handled per-step.
          echo "CC=mpicc"  >> $GITHUB_ENV
          echo "CXX=mpicxx" >> $GITHUB_ENV
          NF_FLIBS="$(nf-config --flibs 2>/dev/null || echo "-L${NETCDF_LIBDIR} -lnetcdff")"
          NC_LIBS="$(nc-config --libs  2>/dev/null || echo "-L${NETCDF_LIBDIR} -lnetcdf")"
          H5_LIBS="$(pkg-config --libs hdf5_hl hdf5 2>/dev/null || echo "-L${HDF5_LIBDIR} -lhdf5_hl -lhdf5")"
          echo "FUSE_LIBRARIES=${NF_FLIBS} ${NC_LIBS} ${H5_LIBS}" >> $GITHUB_ENV
          NF_FFLAGS="$(nf-config --fflags 2>/dev/null || true)"
          NC_CFLAGS="$(nc-config --cflags 2>/dev/null || true)"
          H5_CFLAGS="$(pkg-config --cflags hdf5 2>/dev/null || echo "-I${HDF5_INCLUDE_DIR}")"
          echo "FUSE_INCLUDE=${NC_CFLAGS} ${NF_FFLAGS} ${H5_CFLAGS} -I${NETCDF_INCLUDE}" >> $GITHUB_ENV
          echo "FUSE_FFLAGS_EXTRA=-fallow-argument-mismatch -std=legacy -ffree-line-length-none -fmax-errors=0" >> $GITHUB_ENV

      - name: Create /usr/lib64 symlinks for legacy builds (HDF5/NetCDF)
        run: |
          set -e
          sudo mkdir -p /usr/lib64
          for so in /usr/lib/x86_64-linux-gnu/libnetcdf.so* /usr/lib/x86_64-linux-gnu/libnetcdff.so*; do
            [ -e "$so" ] || continue; base=$(basename "$so")
            [ -e "/usr/lib64/${base}" ] || sudo ln -s "$so" "/usr/lib64/${base}" || true
          done
          for so in /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5.so* \
                    /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5_hl.so* \
                    /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5_fortran.so* \
                    /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5hl_fortran.so*; do
            [ -e "$so" ] || continue; base=$(basename "$so")
            [ -e "/usr/lib64/${base}" ] || sudo ln -s "$so" "/usr/lib64/${base}" || true
          done

      # Reuse your existing install procedure
      - name: Install SYMFLUENCE (wrapper)
        env:
          CC: mpicc
          CXX: mpicxx
          FC: gfortran
          PYTHONIOENCODING: utf-8
        run: |
          set -e
          chmod +x ./scripts/symfluence-bootstrap
          ./scripts/symfluence-bootstrap --install

          # Install test dependencies (pytest moved to optional deps)
          source venv/bin/activate
          pip install -e ".[test]"

      - name: Add external tools to PATH
        run: |
          set -e
          # Add SUMMA
          echo "${{ env.SYMFLUENCE_DATA }}/installs/summa/bin" >> $GITHUB_PATH
          # Add mizuRoute
          echo "${{ env.SYMFLUENCE_DATA }}/installs/mizuRoute/route/bin" >> $GITHUB_PATH
          # Add TauDEM
          echo "${{ env.SYMFLUENCE_DATA }}/installs/TauDEM/bin" >> $GITHUB_PATH

          # Add SUNDIALS library path for SUMMA runtime dependencies
          # NOTE: Do NOT add /usr/lib/x86_64-linux-gnu — it's a standard ldconfig path
          # and adding it to LD_LIBRARY_PATH causes system HDF5 1.10 to override
          # pip netCDF4's bundled HDF5 1.14+ (LD_LIBRARY_PATH beats RUNPATH).
          SUNDIALS_LIB="${{ env.SYMFLUENCE_DATA }}/installs/sundials/install/sundials/lib"
          if [ -d "$SUNDIALS_LIB" ]; then
            echo "LD_LIBRARY_PATH=${SUNDIALS_LIB}" >> $GITHUB_ENV
            echo "Added SUNDIALS library path: $SUNDIALS_LIB"
          else
            echo "SUNDIALS library directory not found at: $SUNDIALS_LIB"
            ls -la "${{ env.SYMFLUENCE_DATA }}/installs/sundials/" || true
          fi
          # Disable HDF5 file locking (common source of errors in CI)
          echo "HDF5_USE_FILE_LOCKING=FALSE" >> $GITHUB_ENV

      - name: Activate venv for subsequent steps
        run: |
          set -e
          echo "VIRTUAL_ENV=${{ env.SYMFLUENCE_CODE }}/venv" >> $GITHUB_ENV
          echo "${{ env.SYMFLUENCE_CODE }}/venv/bin" >> $GITHUB_PATH

      - name: Create CI config for Bow at Banff
        run: |
          set -e
          mkdir -p 0_config_files "${SYMFLUENCE_DATA}"
          cat > 0_config_files/config_ci_bow.yaml <<'YAML'
          SYMFLUENCE_CODE_DIR: "${{ env.SYMFLUENCE_CODE }}"
          SYMFLUENCE_DATA_DIR: "${{ env.SYMFLUENCE_DATA }}"
          DOMAIN_NAME: "${{ env.DOMAIN }}"
          DOMAIN_DEFINITION_METHOD: "${{ env.DOMAIN_DEF }}"
          SUB_GRID_DISCRETIZATION: lumped
          EXPERIMENT_ID: ci_test_001
          EXPERIMENT_TIME_START: 2004-01-01 01:00
          EXPERIMENT_TIME_END: 2004-01-31 23:00
          HYDROLOGICAL_MODEL: SUMMA
          FORCING_DATASET: ERA5
          LOG_LEVEL: "INFO"
          YAML

      - name: Step 1 - setup_project (creates domain scaffolding)
        run: |
          set -e
          ./scripts/symfluence-bootstrap workflow step setup_project --config 0_config_files/config_ci_bow.yaml

      - name: Step 2 - copy repo data into created domain folder
        run: |
          set -e
          # Try both old (src/data) and new (tests/data) locations
          SRC=""
          if [ -d "./tests/data/domain_${DOMAIN}" ]; then
            SRC="./tests/data/domain_${DOMAIN}"
          elif [ -d "./src/data/domain_${DOMAIN}" ]; then
            SRC="./src/data/domain_${DOMAIN}"
          else
            echo "Test data not found for domain: ${DOMAIN}"
            echo "   Checked: ./tests/data/domain_${DOMAIN} and ./src/data/domain_${DOMAIN}"
            echo "   Skipping data copy step"
            SRC=""
          fi

          if [ -n "${SRC}" ]; then
            DEST="${SYMFLUENCE_DATA}/domain_${DOMAIN}"
            mkdir -p "${DEST}"
            # Copy everything (shapefiles, any sidecars, etc.)
            cp -a "${SRC}/." "${DEST}/" || true
            echo "Copied data into ${DEST}"
            ls -R "${DEST}" || true
          fi

      - name: Step 3 - run define_domain using CLI (if pour_point data available)
        run: |
          set -e
          DOMAIN_DIR="${SYMFLUENCE_DATA}/domain_${DOMAIN}"
          POUR_POINT_FILE="${DOMAIN_DIR}/shapefiles/pour_point/${DOMAIN}_pourPoint.shp"

          if [ -f "${POUR_POINT_FILE}" ]; then
            echo "Pour point shapefile found, running define_domain..."
            ./scripts/symfluence-bootstrap workflow step define_domain --config 0_config_files/config_ci_bow.yaml
          else
            echo "Pour point shapefile not found at: ${POUR_POINT_FILE}"
            echo "   Skipping define_domain step (test data not available)"
            echo "   To enable this step, provide pour_point shapefiles in tests/data/domain_${DOMAIN}/shapefiles/pour_point/"
          fi

      - name: Assertions - check outputs (if available)
        run: |
          set -e
          DOMAIN_DIR="${SYMFLUENCE_DATA}/domain_${DOMAIN}"

          # Check if workflow logs exist (always created)
          if ls "_workLog_${DOMAIN}" >/dev/null 2>&1; then
            echo "Workflow logs found"
          else
            echo "No workflow logs found (define_domain may have been skipped)"
          fi

          # Check river basins only if pour_point was available
          RIV_DIR="${DOMAIN_DIR}/shapefiles/river_basins"
          if ls "${RIV_DIR}/${DOMAIN}_riverBasins_"*.shp >/dev/null 2>&1; then
            echo "River basins shapefile found"
          else
            echo "River basins shapefile not found (define_domain may have been skipped)"
          fi

      - name: Upload artifacts on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: domain-define-minimal-logs
          path: |
            _workLog_${{ env.DOMAIN }}/**
            ${{ env.SYMFLUENCE_DATA }}/domain_${{ env.DOMAIN }}/**/*
          if-no-files-found: warn
