{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# SYMFLUENCE Tutorial 04a — Logan River Workshop (Lumped SUMMA, Cloud Data)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This workshop notebook demonstrates how to set up a lumped SUMMA model for the Logan River at Logan, Utah using cloud-based data sources. The workflow includes:\n",
    "\n",
    "1. **Configuration** — Set up a lumped basin model for the Logan River\n",
    "2. **Domain Definition** — Delineate the watershed using TauDEM\n",
    "3. **Data Acquisition** — Fetch AORC forcing data and USGS streamflow observations from cloud sources\n",
    "4. **Model Execution** — Run SUMMA with mizuRoute routing\n",
    "5. **Evaluation & Calibration** — Assess model performance and calibrate parameters\n",
    "\n",
    "The **Logan River at Logan** is a snow-dominated mountain watershed in the Bear River Range of the Wasatch Mountains. USGS station 10109000 provides streamflow observations. The watershed covers approximately 218 km² with elevations ranging from ~1,400 m to over 2,900 m.\n",
    "\n",
    "### 2i2c Environment Setup\n",
    "\n",
    "This notebook is designed to work with the 2i2c JupyterHub environment using the pre-installed SYMFLUENCE virtual environment at `/tmp/symfluence`.\n",
    "\n",
    "**Launch this notebook from the CLI:**\n",
    "```bash\n",
    "symfluence example launch 4a\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment verification\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress experimental module warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', message='.*is an EXPERIMENTAL module.*')\n",
    "warnings.filterwarnings('ignore', message='.*import failed.*')\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Verify SYMFLUENCE is available\n",
    "try:\n",
    "    import symfluence\n",
    "    print(f\"SYMFLUENCE version: {symfluence.__version__}\")\n",
    "    print(f\"SYMFLUENCE location: {Path(symfluence.__file__).parent}\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: SYMFLUENCE not found. Please activate the symfluence environment.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix working directory if running from .ipynb_checkpoints\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "\n",
    "# If we're in .ipynb_checkpoints, move up to parent directory\n",
    "if '.ipynb_checkpoints' in str(current_dir):\n",
    "    correct_dir = current_dir.parent\n",
    "    os.chdir(correct_dir)\n",
    "    print(f\"Changed to: {Path.cwd()}\")\n",
    "else:\n",
    "    print(\"Working directory is correct\")\n",
    "\n",
    "# Verify we're in the workshop notebooks directory\n",
    "expected_notebook = Path.cwd() / '04a_logan_river_workshop.ipynb'\n",
    "if not expected_notebook.exists():\n",
    "    print(f\"WARNING: Expected notebook not found at {expected_notebook}\")\n",
    "else:\n",
    "    print(\"Notebook location verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 1 — Configuration\n",
    "\n",
    "Create a configuration for the Logan River lumped basin model. Key settings:\n",
    "- **Domain**: Lumped (single HRU) representation\n",
    "- **Forcing**: AORC (Analysis of Record for Calibration) — 1km hourly gridded data\n",
    "- **Observations**: USGS streamflow from station 10109000\n",
    "- **Period**: 4 years (2018-2021) with 1-year spinup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Create basin-scale configuration using new config system\n",
    "\n",
    "from pathlib import Path\n",
    "from symfluence import SYMFLUENCE\n",
    "from symfluence.core.config.models import SymfluenceConfig\n",
    "import os\n",
    "\n",
    "# === Logan River Basin Configuration ===\n",
    "# Using the new config factory method with modern lowercase/snake_case syntax\n",
    "\n",
    "# Ensure we're working from the correct directory\n",
    "current_dir = Path.cwd()\n",
    "print(f\"INITIAL working directory: {current_dir}\")\n",
    "\n",
    "if '.ipynb_checkpoints' in str(current_dir):\n",
    "    current_dir = current_dir.parent\n",
    "    os.chdir(current_dir)\n",
    "    print(f\"CHANGED to: {Path.cwd()}\")\n",
    "else:\n",
    "    print(\"Working directory is correct\")\n",
    "\n",
    "# Set explicit paths\n",
    "# Navigate to repo root (2 levels up from notebooks directory)\n",
    "repo_root = current_dir.parent.parent\n",
    "data_dir = repo_root.parent / 'SYMFLUENCE_data'  # Sibling to repo\n",
    "\n",
    "taudem_path = str(data_dir / 'installs' / 'TauDEM' / 'bin')\n",
    "mizuroute_path = '/Users/darrieythorsson/compHydro/data/CONFLUENCE_data/installs/mizuRoute/route/bin'\n",
    "\n",
    "print(f\"Notebook directory: {current_dir}\")\n",
    "print(f\"Repo root (CODE_DIR): {repo_root}\")\n",
    "print(f\"Data directory (DATA_DIR): {data_dir}\")\n",
    "print(f\"TauDEM directory: {taudem_path}\")\n",
    "print(f\"mizuRoute directory: {mizuroute_path}\")\n",
    "\n",
    "config = SymfluenceConfig.from_minimal(\n",
    "    # ============================================================================\n",
    "    # BASIC IDENTIFICATION\n",
    "    # ============================================================================\n",
    "    domain_name='Logan_River_at_Logan',\n",
    "    experiment_id='workshop_run_1',\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PATHS (explicit to avoid working directory issues)\n",
    "    # Use UPPERCASE parameter names as required by from_minimal\n",
    "    # ============================================================================\n",
    "    SYMFLUENCE_DATA_DIR=str(data_dir),\n",
    "    SYMFLUENCE_CODE_DIR=str(repo_root),\n",
    "    TAUDEM_DIR=taudem_path,\n",
    "    SUMMA_INSTALL_PATH='default',  # Will use DATA_DIR/installs/summa/bin/summa_sundials.exe\n",
    "    MIZUROUTE_INSTALL_PATH=mizuroute_path,\n",
    "    \n",
    "    # ============================================================================\n",
    "    # HYDROLOGICAL MODEL\n",
    "    # ============================================================================\n",
    "    model='SUMMA',                              # Hydrological model (sets defaults)\n",
    "    routing_model='mizuRoute',                  # Routing model\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SIMULATION PERIOD (4 years: 2018-2021)\n",
    "    # ============================================================================\n",
    "    time_start='2018-01-01 01:00',\n",
    "    time_end='2021-12-31 23:00',\n",
    "    \n",
    "    # Time period definitions\n",
    "    spinup_period='2018-01-01, 2018-12-31',        # Year 1: Model spinup\n",
    "    calibration_period='2019-01-01, 2020-12-31',   # Years 2-3: Parameter calibration\n",
    "    evaluation_period='2021-01-01, 2021-12-31',    # Year 4: Model evaluation\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SPATIAL DOMAIN (Logan River at Logan, UT)\n",
    "    # ============================================================================\n",
    "    # USGS station 10109000: 41.7443°N, 111.8086°W\n",
    "    pour_point_coords='41.743098/-111.786432',\n",
    "    bounding_box_coords='42.15/-111.90/41.70/-111.40',  # lat_max/lon_min/lat_min/lon_max\n",
    "    \n",
    "    # Domain discretization\n",
    "    definition_method='lumped',\n",
    "    discretization='GRUs',\n",
    "    lumped_watershed_method='TauDEM',\n",
    "    \n",
    "    # ============================================================================\n",
    "    # DATA SOURCES & FORCING\n",
    "    # ============================================================================\n",
    "    data_access='cloud',                        # Use cloud-based data sources\n",
    "    forcing_dataset='AORC',                     # NOAA Analysis of Record for Calibration\n",
    "    forcing_measurement_height=10,              # AORC wind measurements at 10m\n",
    "    \n",
    "    # DEM source (use Copernicus - free and open)\n",
    "    dem_source='copernicus',                    # Copernicus DEM (30m, global, free)\n",
    "    download_dem=True,\n",
    "    \n",
    "    # ============================================================================\n",
    "    # STREAMFLOW OBSERVATIONS\n",
    "    # ============================================================================\n",
    "    station_id='10109000',                      # USGS station ID\n",
    "    streamflow_data_provider='USGS',\n",
    "    download_usgs_data=True,\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CALIBRATION SETTINGS\n",
    "    # ============================================================================\n",
    "    # Enable optimization methods\n",
    "    OPTIMIZATION_METHODS=['iteration'],         # Enable iterative optimization\n",
    "    \n",
    "    # Parameters to calibrate\n",
    "    params_to_calibrate='k_soil,theta_sat,aquiferBaseflowExp,aquiferBaseflowRate,qSurfScale,summerLAI,frozenPrecipMultip,Fcapil,tempCritRain,heightCanopyTop,heightCanopyBottom,windReductionParam,vGn_n',\n",
    "    basin_params_to_calibrate='routingGammaScale,routingGammaShape',\n",
    "    \n",
    "    # Optimization configuration\n",
    "    optimization_target='streamflow',\n",
    "    optimization_algorithm='DDS',               # Dynamically Dimensioned Search\n",
    "    optimization_metric='KGE',                  # Kling-Gupta Efficiency\n",
    "    calibration_timestep='hourly',\n",
    "    iterations=100,                             # Number of calibration iterations\n",
    ")\n",
    "\n",
    "# Verify the config was created correctly\n",
    "print(f\"\\nConfig verification:\")\n",
    "print(f\"  Config taudem_dir: {config.paths.taudem_dir}\")\n",
    "print(f\"  Optimization methods: {config.optimization.methods}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE CONFIGURATION (OPTIONAL)\n",
    "# ============================================================================\n",
    "config_path = Path('./config_logan_river_lumped.yaml')\n",
    "config_dict = config.to_dict(flatten=True)\n",
    "import yaml\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f, default_flow_style=False, sort_keys=False)\n",
    "print(f\"\\nConfiguration saved to: {config_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# INITIALIZE SYMFLUENCE\n",
    "# ============================================================================\n",
    "symfluence = SYMFLUENCE(config)\n",
    "\n",
    "# Create project structure\n",
    "project_dir = symfluence.managers['project'].setup_project()\n",
    "pour_point_path = symfluence.managers['project'].create_pour_point()\n",
    "\n",
    "print(f\"\\nProject structure created at: {project_dir}\")\n",
    "print(f\"Pour point shapefile: {pour_point_path}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 2 — Domain Definition\n",
    "\n",
    "Delineate the Logan River watershed using TauDEM and create a single lumped HRU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Step 2a — Geospatial Attribute Acquisition\n",
    "\n",
    "Acquire elevation, land cover, and soil data from cloud sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2a — Acquire geospatial attributes from cloud\n",
    "symfluence.managers['data'].acquire_attributes()\n",
    "print(\"Attribute acquisition complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Step 2b — Watershed Delineation\n",
    "\n",
    "Delineate the watershed boundary from the pour point using TauDEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2b — Watershed delineation\n",
    "watershed_path = symfluence.managers['domain'].define_domain()\n",
    "print(f\"Watershed delineation complete\")\n",
    "print(f\"Watershed file: {watershed_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Step 2c — Domain Discretization\n",
    "\n",
    "Create a single lumped HRU for the watershed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2c — Discretization (single lumped HRU)\n",
    "hru_path = symfluence.managers['domain'].discretize_domain()\n",
    "print(\"Domain discretization complete\")\n",
    "print(f\"HRU file: {hru_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Step 2d — Visualization\n",
    "\n",
    "Visualize the delineated watershed and pour point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2d — Basin visualization\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load spatial data\n",
    "basin_path = project_dir / 'shapefiles' / 'river_basins' / f\"{config.domain.name}_riverBasins_lumped.shp\"\n",
    "hru_file = project_dir / 'shapefiles' / 'catchment' / 'lumped' / config.domain.experiment_id / f\"{config.domain.name}_HRUs_GRUs.shp\"                     \n",
    "\n",
    "watershed_gdf = gpd.read_file(str(basin_path))\n",
    "hru_gdf = gpd.read_file(str(hru_file))\n",
    "pour_point_gdf = gpd.read_file(pour_point_path)\n",
    "\n",
    "# Calculate area (UTM Zone 12N for Utah)\n",
    "watershed_proj = watershed_gdf.to_crs('EPSG:32612')\n",
    "area_km2 = watershed_proj.geometry.area.sum() / 1e6\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "watershed_gdf.boundary.plot(ax=ax, color='blue', linewidth=2, label='Watershed')\n",
    "hru_gdf.plot(ax=ax, facecolor='lightblue', edgecolor='blue', alpha=0.3)\n",
    "pour_point_gdf.plot(ax=ax, color='red', markersize=150, marker='*', label=f'Pour Point (USGS {config.evaluation.streamflow.station_id})')\n",
    "\n",
    "ax.set_title(f\"Logan River at Logan\\nArea: {area_km2:.0f} km²\", fontweight='bold', fontsize=14)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Watershed area: {area_km2:.0f} km²\")\n",
    "print(f\"Number of HRUs: {len(hru_gdf)} (lumped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Step 3 — Data Acquisition and Preprocessing\n",
    "\n",
    "Fetch forcing data (AORC) and streamflow observations (USGS) from cloud sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Step 3a — USGS Streamflow Observations\n",
    "\n",
    "Download and process USGS streamflow data for station 10109000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a — Download and process USGS streamflow data\n",
    "symfluence.managers['data'].process_observed_data()                                                                                                      \n",
    "\n",
    "print(\"USGS streamflow data acquisition complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Step 3b — AORC Meteorological Forcing\n",
    "\n",
    "Download AORC forcing data from NOAA's cloud archive (AWS S3). AORC provides:\n",
    "- 1 km spatial resolution\n",
    "- Hourly temporal resolution\n",
    "- Complete forcing variables: precipitation, temperature, humidity, wind, radiation, pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b — Acquire AORC forcing data from cloud\n",
    "symfluence.managers['data'].acquire_forcings()\n",
    "print(\"AORC forcing acquisition complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Step 3c — Model-Agnostic Preprocessing\n",
    "\n",
    "Standardize forcing data: variable names, units, and spatial averaging over the watershed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3c — Model-agnostic preprocessing\n",
    "symfluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "print(\"Model-agnostic preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Step 4 — Model Configuration and Execution\n",
    "\n",
    "Configure SUMMA for the lumped basin and run the simulation with mizuRoute routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4a — SUMMA-specific preprocessing\n",
    "symfluence.managers['model'].preprocess_models()\n",
    "print(\"SUMMA configuration complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4b — Model execution\n",
    "print(f\"Running {config.model.hydrological_model} with {config.model.routing_model}...\")\n",
    "print(f\"Simulation period: {config.domain.time_start} to {config.domain.time_end}\")\n",
    "symfluence.managers['model'].run_models()\n",
    "print(\"Basin-scale simulation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Step 5 — Streamflow Evaluation\n",
    "\n",
    "Compare simulated streamflow against USGS observations using standard hydrological metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 — Streamflow evaluation\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "# Load basin area from shapefile\n",
    "basin_path = project_dir / 'shapefiles' / 'river_basins' / f\"{config.domain.name}_riverBasins_lumped.shp\"\n",
    "watershed_gdf = gpd.read_file(str(basin_path))\n",
    "watershed_proj = watershed_gdf.to_crs('EPSG:32612')  # UTM Zone 12N for Utah\n",
    "basin_area_m2 = watershed_proj.geometry.area.sum()\n",
    "basin_area_km2 = basin_area_m2 / 1e6\n",
    "\n",
    "print(f\"Basin area: {basin_area_km2:.2f} km²\")\n",
    "\n",
    "# Load observed streamflow\n",
    "obs_path = project_dir / \"observations\" / \"streamflow\" / \"preprocessed\" / f\"{config.domain.name}_streamflow_processed.csv\"\n",
    "obs_df = pd.read_csv(obs_path, parse_dates=['datetime'])\n",
    "obs_df.set_index('datetime', inplace=True)\n",
    "\n",
    "# Load simulated streamflow from SUMMA output\n",
    "sim_dir = project_dir / \"simulations\" / config.domain.experiment_id / \"SUMMA\"\n",
    "sim_files = list(sim_dir.glob('*_timestep.nc'))\n",
    "if not sim_files:\n",
    "    raise FileNotFoundError(f\"No SUMMA output found in: {sim_dir}\")\n",
    "\n",
    "sim_ds = xr.open_dataset(sim_files[0])\n",
    "sim_df = sim_ds['averageRoutedRunoff'].to_dataframe().reset_index()\n",
    "sim_df = sim_df.rename(columns={'time': 'datetime', 'averageRoutedRunoff': 'discharge_m_s'})\n",
    "sim_df.set_index('datetime', inplace=True)\n",
    "\n",
    "# Convert from m/s to m³/s\n",
    "sim_df['discharge_sim'] = sim_df['discharge_m_s'] * basin_area_m2\n",
    "\n",
    "# Exclude spinup period\n",
    "spinup_end = pd.to_datetime(config.domain.spinup_period.split(',')[1].strip())\n",
    "print(f\"Excluding spinup period up to: {spinup_end}\")\n",
    "\n",
    "# Merge and align\n",
    "eval_df = obs_df.join(sim_df[['discharge_sim']], how='inner')\n",
    "eval_df = eval_df[eval_df.index > spinup_end]\n",
    "\n",
    "obs_valid = eval_df['discharge_cms'].dropna()\n",
    "sim_valid = eval_df.loc[obs_valid.index, 'discharge_sim']\n",
    "\n",
    "print(f\"Evaluation period: {obs_valid.index[0]} to {obs_valid.index[-1]}\")\n",
    "print(f\"Number of timesteps: {len(obs_valid)}\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "def nse(obs, sim):\n",
    "    return float(1 - np.sum((obs - sim)**2) / np.sum((obs - obs.mean())**2))\n",
    "\n",
    "def kge(obs, sim):\n",
    "    r = obs.corr(sim)\n",
    "    alpha = sim.std() / obs.std()\n",
    "    beta = sim.mean() / obs.mean()\n",
    "    return float(1 - np.sqrt((r-1)**2 + (alpha-1)**2 + (beta-1)**2))\n",
    "\n",
    "def pbias(obs, sim):\n",
    "    return float(100 * (sim.sum() - obs.sum()) / obs.sum())\n",
    "\n",
    "metrics = {\n",
    "    'NSE': round(nse(obs_valid, sim_valid), 3),\n",
    "    'KGE': round(kge(obs_valid, sim_valid), 3),\n",
    "    'PBIAS': round(pbias(obs_valid, sim_valid), 1)\n",
    "}\n",
    "\n",
    "print(\"\\nPerformance Metrics (Uncalibrated):\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Time series (top left)\n",
    "axes[0, 0].plot(obs_valid.index, obs_valid.values, 'b-', label='Observed (USGS)', linewidth=1.2, alpha=0.7)\n",
    "axes[0, 0].plot(sim_valid.index, sim_valid.values, 'r-', label='Simulated (SUMMA)', linewidth=1.2, alpha=0.7)\n",
    "axes[0, 0].set_ylabel('Discharge (m³/s)')\n",
    "axes[0, 0].set_title('Streamflow Time Series')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].text(0.02, 0.95, f\"NSE: {metrics['NSE']}\\nKGE: {metrics['KGE']}\\nBias: {metrics['PBIAS']}%\",\n",
    "                transform=axes[0, 0].transAxes, verticalalignment='top',\n",
    "                bbox=dict(facecolor='white', alpha=0.8), fontsize=9)\n",
    "\n",
    "# Scatter (top right)\n",
    "axes[0, 1].scatter(obs_valid, sim_valid, alpha=0.5, s=10)\n",
    "max_val = max(obs_valid.max(), sim_valid.max())\n",
    "axes[0, 1].plot([0, max_val], [0, max_val], 'k--', alpha=0.5)\n",
    "axes[0, 1].set_xlabel('Observed (m³/s)')\n",
    "axes[0, 1].set_ylabel('Simulated (m³/s)')\n",
    "axes[0, 1].set_title('Observed vs Simulated')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly climatology (bottom left)\n",
    "monthly_obs = obs_valid.groupby(obs_valid.index.month).mean()\n",
    "monthly_sim = sim_valid.groupby(sim_valid.index.month).mean()\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "axes[1, 0].plot(monthly_obs.index, monthly_obs.values, 'b-o', label='Observed', markersize=6)\n",
    "axes[1, 0].plot(monthly_sim.index, monthly_sim.values, 'r-o', label='Simulated', markersize=6)\n",
    "axes[1, 0].set_xticks(range(1, 13))\n",
    "axes[1, 0].set_xticklabels(month_names)\n",
    "axes[1, 0].set_ylabel('Mean Discharge (m³/s)')\n",
    "axes[1, 0].set_title('Seasonal Flow Regime')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Flow duration curve (bottom right)\n",
    "obs_sorted = obs_valid.sort_values(ascending=False)\n",
    "sim_sorted = sim_valid.sort_values(ascending=False)\n",
    "obs_ranks = np.arange(1., len(obs_sorted) + 1) / len(obs_sorted) * 100\n",
    "sim_ranks = np.arange(1., len(sim_sorted) + 1) / len(sim_sorted) * 100\n",
    "axes[1, 1].semilogy(obs_ranks, obs_sorted, 'b-', label='Observed', linewidth=2)\n",
    "axes[1, 1].semilogy(sim_ranks, sim_sorted, 'r-', label='Simulated', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Exceedance Probability (%)')\n",
    "axes[1, 1].set_ylabel('Discharge (m³/s)')\n",
    "axes[1, 1].set_title('Flow Duration Curve')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Logan River at Logan — Lumped SUMMA Evaluation', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStreamflow evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Step 5b — Model Calibration\n",
    "\n",
    "Calibrate SUMMA parameters using the DDS (Dynamically Dimensioned Search) algorithm to improve model performance. The calibration optimizes KGE over the calibration period (2019-2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5b — Run calibration\n",
    "print(f\"Starting calibration...\")\n",
    "print(f\"Algorithm: {config.optimization.algorithm}\")\n",
    "print(f\"Metric: {config.optimization.metric}\")\n",
    "print(f\"Iterations: {config.optimization.iterations}\")\n",
    "print(f\"Calibration period: {config.domain.calibration_period}\")\n",
    "\n",
    "results_file = symfluence.managers['optimization'].calibrate_model()\n",
    "print(f\"\\nCalibration complete!\")\n",
    "print(f\"Results file: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### View Calibration Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display calibration results\n",
    "if results_file and Path(results_file).exists():\n",
    "    results_df = pd.read_csv(results_file)\n",
    "    \n",
    "    print(\"Calibration Progress:\")\n",
    "    print(f\"  Best {config.optimization.metric}: {results_df['best_score'].iloc[-1]:.4f}\")\n",
    "    print(f\"  Initial {config.optimization.metric}: {results_df['best_score'].iloc[0]:.4f}\")\n",
    "    print(f\"  Improvement: {results_df['best_score'].iloc[-1] - results_df['best_score'].iloc[0]:.4f}\")\n",
    "    \n",
    "    # Plot calibration progress\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(results_df['generation'], results_df['best_score'], 'b-', linewidth=2)\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel(f'Best {config.optimization.metric}')\n",
    "    ax.set_title('Calibration Progress')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No calibration results found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (symfluence-root)",
   "language": "python",
   "name": "symfluence-root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
