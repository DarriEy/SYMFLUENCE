{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYMFLUENCE Tutorial 1a — Point-Scale Workflow (Paradise SNOTEL)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the point-scale modeling workflow in **SYMFLUENCE**, a framework for reproducible and modular computational hydrology. At the point scale, we simulate vertical energy and water fluxes at a single site, independent of routing or lateral flow, to isolate and evaluate model process representations.\n",
    "\n",
    "Here, we focus on the **Paradise SNOTEL station (ID 602)**, located at 1,630 m elevation in Washington’s Cascade Range. This site represents a transitional snow climate and provides long-term observations of snow water equivalent (SWE) and soil moisture across multiple depths. By reproducing the observed seasonal snow and soil moisture dynamics, this tutorial demonstrates how SYMFLUENCE structures a controlled, transparent, and fully reproducible point-scale experiment.\n",
    "\n",
    "Through this example, you will see how configuration-driven workflows manage experiment setup, geospatial definition, input data preprocessing, model instantiation, and performance evaluation—building a foundation for more complex distributed modeling studies later in the series.\n",
    "\n",
    "### What you will learn\n",
    "\n",
    "1. **Typed configuration** — Create a validated config using `SymfluenceConfig.from_minimal()`\n",
    "2. **Point-scale domain** — Define a single-GRU domain around a SNOTEL station\n",
    "3. **Input preprocessing** — Acquire forcings, observations, and run model-agnostic preprocessing\n",
    "4. **Model execution** — Run SUMMA for a point-scale simulation\n",
    "5. **Calibration** — Calibrate snow parameters using Differential Evolution (DE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup — HDF5 locking workaround and verification\n",
    "#\n",
    "# The HDF5_USE_FILE_LOCKING='FALSE' setting prevents file-locking errors\n",
    "# that can occur on certain filesystems (NFS, FUSE, conda-managed HDF stacks).\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "\n",
    "warnings.filterwarnings('ignore', message='.*is an EXPERIMENTAL module.*')\n",
    "warnings.filterwarnings('ignore', message='.*import failed.*')\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "try:\n",
    "    import symfluence\n",
    "    print(f\"SYMFLUENCE version: {symfluence.__version__}\")\n",
    "    print(f\"SYMFLUENCE location: {Path(symfluence.__file__).parent}\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: SYMFLUENCE not found. Please activate the symfluence environment.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 — Configuration\n",
    "\n",
    "We create a typed, validated configuration using `SymfluenceConfig.from_minimal()`. This replaces the legacy YAML-template workflow with a single factory call that provides:\n",
    "\n",
    "- **Type safety** — Pydantic validates all fields at creation time\n",
    "- **Smart defaults** — Model-specific defaults are applied automatically\n",
    "- **Immutability** — Configuration is frozen after creation (thread-safe, reproducible)\n",
    "\n",
    "> **Note:** All configuration is frozen once created. If you need to change a setting, you must re-create the config and re-initialize SYMFLUENCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1a — Create typed configuration for Paradise SNOTEL\n",
    "\n",
    "from pathlib import Path\n",
    "from symfluence.core.config.models import SymfluenceConfig\n",
    "\n",
    "# Resolve code and data directories\n",
    "# Convention: SYMFLUENCE_data sits alongside the SYMFLUENCE repo directory\n",
    "repo_root = Path(__file__).resolve().parent.parent.parent if '__file__' in dir() else Path.cwd().resolve()\n",
    "# Walk up until we find the repo root (contains src/symfluence)\n",
    "while repo_root != repo_root.parent:\n",
    "    if (repo_root / 'src' / 'symfluence').exists():\n",
    "        break\n",
    "    repo_root = repo_root.parent\n",
    "\n",
    "data_dir = repo_root.parent / 'SYMFLUENCE_data'\n",
    "if not data_dir.exists():\n",
    "    # Fallback: check environment variable or use default\n",
    "    import os\n",
    "    data_dir = Path(os.getenv('SYMFLUENCE_DATA_DIR', str(repo_root / 'data')))\n",
    "\n",
    "print(f\"Repo root: {repo_root}\")\n",
    "print(f\"Data dir:  {data_dir}\")\n",
    "\n",
    "config = SymfluenceConfig.from_minimal(\n",
    "    # === Identification ===\n",
    "    domain_name='paradise',\n",
    "    experiment_id='run_1',\n",
    "\n",
    "    # === Paths ===\n",
    "    SYMFLUENCE_DATA_DIR=str(data_dir),\n",
    "    SYMFLUENCE_CODE_DIR=str(repo_root),\n",
    "\n",
    "    # === Model & Forcing ===\n",
    "    model='SUMMA',\n",
    "    forcing_dataset='ERA5',\n",
    "\n",
    "    # === Spatial Domain (point-scale) ===\n",
    "    definition_method='point',\n",
    "    discretization='GRUs',\n",
    "    pour_point_coords='46.78/-121.75',\n",
    "    bounding_box_coords='46.781/-121.751/46.779/-121.749',\n",
    "\n",
    "    # === Temporal Extent ===\n",
    "    time_start='2000-01-01 01:00',\n",
    "    time_end='2002-12-31 23:00',\n",
    "    spinup_period='2000-01-01, 2000-09-30',\n",
    "    calibration_period='2000-10-01, 2001-09-30',\n",
    "    evaluation_period='2001-10-01, 2002-09-30',\n",
    "\n",
    "    # === Observations ===\n",
    "    DOWNLOAD_SNOTEL=True,\n",
    "    SNOTEL_STATION='602',                       # Paradise SNOTEL (ID 602)\n",
    "\n",
    "    # === Calibration (Differential Evolution) ===\n",
    "    # DE is a generation-based algorithm that evaluates a full population each\n",
    "    # iteration, making it robust to occasional model failures from poor\n",
    "    # parameter combinations — unlike DDS which starts from a single point.\n",
    "    OPTIMIZATION_METHODS=['iteration'],\n",
    "    optimization_algorithm='DE',\n",
    "    optimization_metric='KGE',\n",
    "    optimization_target='swe',\n",
    "    calibration_timestep='daily',\n",
    "    iterations=20,\n",
    "    POPULATION_SIZE=10,\n",
    "    PARAMS_TO_CALIBRATE='tempCritRain,tempRangeTimestep,frozenPrecipMultip,albedoMax,albedoMinWinter,albedoDecayRate,k_soil,vGn_n,theta_sat',\n",
    ")\n",
    "\n",
    "print(f\"\\nDomain:      {config.domain.name}\")\n",
    "print(f\"Model:       {config.model.hydrological_model}\")\n",
    "print(f\"Forcing:     {config.forcing.dataset}\")\n",
    "print(f\"Period:      {config.domain.time_start} to {config.domain.time_end}\")\n",
    "print(f\"Algorithm:   {config.optimization.algorithm}\")\n",
    "print(f\"Metric:      {config.optimization.metric}\")\n",
    "print(f\"Data dir:    {config.system.data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b — Download Example Data (Optional)\n",
    "\n",
    "You can download pre-processed example data from GitHub releases. This step downloads and extracts the example data to your `SYMFLUENCE_DATA_DIR`.\n",
    "\n",
    "If you already have the example data, skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1c — Initialize SYMFLUENCE & Project Setup\n",
    "\n",
    "Initialize the framework and create the standardized project directory. The config object is passed directly (no intermediate YAML file needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1c — Initialize SYMFLUENCE and create project structure\n",
    "\n",
    "from symfluence import SYMFLUENCE\n",
    "\n",
    "symfluence = SYMFLUENCE(config, visualize=True)\n",
    "\n",
    "# Create standardized project layout and pour-point feature\n",
    "project_dir = symfluence.managers['project'].setup_project()\n",
    "pour_point_path = symfluence.managers['project'].create_pour_point()\n",
    "\n",
    "print(f\"Project root: {project_dir}\")\n",
    "print(f\"Pour point:   {pour_point_path}\")\n",
    "\n",
    "# Brief top-level directory preview\n",
    "print(\"\\nTop-level structure:\")\n",
    "for p in sorted(Path(project_dir).iterdir()):\n",
    "    if p.is_dir():\n",
    "        print(f\"├── {p.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 — Domain Definition (point-scale GRU)\n",
    "\n",
    "For the Paradise SNOTEL example, the domain is a **single GRU** representing the site footprint.  \n",
    "This keeps the workflow strictly point-scale (no routing), aligning the geometry with the pour point created in Step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2a — Geospatial Attribute Acquisition\n",
    "\n",
    "Acquire site attributes (elevation, land cover, soils, etc.). These are model-agnostic inputs used to parameterize vertical energy and water balance at the site.\n",
    "\n",
    "- Uncomment the acquisition line below to download data (set `DATA_ACCESS` to `'cloud'` or `'maf'` in config)\n",
    "- Alternatively, copy pre-downloaded attributes, forcing, and observation directories into the domain directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2a — Acquire attributes (model-agnostic)\n",
    "# Uncomment to acquire data (set DATA_ACCESS to 'cloud' or 'maf' in config)\n",
    "# symfluence.managers['data'].acquire_attributes()\n",
    "print(\"Attribute acquisition complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b — Domain Definition (point-scale)\n",
    "\n",
    "With attributes prepared, we define a point-scale domain consistent with the pour point.  \n",
    "For this example, the domain is a minimal footprint around the Paradise SNOTEL site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2b — Define the point-scale domain\n",
    "watershed_path = symfluence.managers['domain'].define_domain()\n",
    "print(f\"Domain definition complete\")\n",
    "print(f\"Domain file: {watershed_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2c — Discretization\n",
    "\n",
    "Discretization writes the **catchment HRU shapefile** and related artifacts required by downstream steps.  \n",
    "For the point-scale case we set `discretization='GRUs'`, which creates a **single HRU** identical to the GRU while still generating the standardized outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2c — Discretization (GRUs → HRUs 1:1)\n",
    "hru_path = symfluence.managers['domain'].discretize_domain()\n",
    "print(f\"Domain discretization complete\")\n",
    "print(f\"HRU file: {hru_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2d — Verification & Visualization\n",
    "\n",
    "Verify that discretization produced the expected shapefiles and plot the GRU–HRU overlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2d — Verify domain outputs and visualize\n",
    "from IPython.display import Image, display\n",
    "\n",
    "plot_path = symfluence.managers['domain'].visualize_domain()\n",
    "print(f\"Domain plot saved to: {plot_path}\")\n",
    "\n",
    "if plot_path:\n",
    "    display(Image(filename=str(plot_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 — Input Preprocessing (model-agnostic)\n",
    "\n",
    "We prepare inputs in three steps:\n",
    "1. Acquire **meteorological forcings** (ERA5)\n",
    "2. Process **observations** (SNOTEL SWE and soil moisture)\n",
    "3. Run **model-agnostic preprocessing** to standardize time steps, variables, and units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3a — Acquire Meteorological Forcings (ERA5)\n",
    "\n",
    "Downloads/subsets the forcings for the Paradise domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a — Forcings\n",
    "# Uncomment to acquire data (set DATA_ACCESS to 'cloud' or 'maf' in config)\n",
    "# symfluence.managers['data'].acquire_forcings()\n",
    "print(\"Forcing data acquisition complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3b — Process Observations (SNOTEL)\n",
    "\n",
    "Parses site observations (SWE, soil moisture), applies basic QA/QC, and stores standardized outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b — Observations\n",
    "# Uncomment to download and process observations\n",
    "# symfluence.managers['data'].process_observed_data()\n",
    "print(\"Observational data processing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3c — Model-Agnostic Preprocessing\n",
    "\n",
    "Standardizes variable names, units, and time steps so multiple models can consume the same inputs consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3c — Model-agnostic preprocessing\n",
    "symfluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "print(\"Model-agnostic preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3d — Verification\n",
    "\n",
    "Confirm the expected folders exist and contain files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3d — Verify preprocessing outputs\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "domain_dir = config.system.data_dir / f\"domain_{config.domain.name}\"\n",
    "\n",
    "targets = {\n",
    "    \"forcing/raw_data\":                     domain_dir / \"forcing\" / \"raw_data\",\n",
    "    \"forcing/basin_averaged_data\":          domain_dir / \"forcing\" / \"basin_averaged_data\",\n",
    "    \"observations/snow/raw\":                domain_dir / \"observations\" / \"snow\" / \"swe\" / \"raw\",\n",
    "    \"observations/snow/processed\":          domain_dir / \"observations\" / \"snow\" / \"swe\" / \"processed\",\n",
    "    \"observations/soil_moisture/raw\":       domain_dir / \"observations\" / \"soil_moisture\" / \"ismn\" / \"raw\",\n",
    "    \"observations/soil_moisture/processed\": domain_dir / \"observations\" / \"soil_moisture\" / \"ismn\" / \"processed\",\n",
    "}\n",
    "\n",
    "def count_files(p: Path) -> int:\n",
    "    return sum(1 for x in p.iterdir() if x.is_file()) if p.exists() else 0\n",
    "\n",
    "for label, path in targets.items():\n",
    "    exists = path.exists()\n",
    "    n = count_files(path)\n",
    "    status = \"OK\" if exists and n > 0 else (\"empty\" if exists else \"missing\")\n",
    "    suffix = f\"({n} files)\" if exists else \"\"\n",
    "    print(f\"[{status:>7}] {label}  {suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 — Model-Specific Preprocessing & Run (SUMMA)\n",
    "\n",
    "We now convert the model-agnostic inputs into **SUMMA-ready inputs**, then run the model for the Paradise point-scale case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4a — SUMMA-Specific Preprocessing\n",
    "\n",
    "Creates the SUMMA input bundle (metadata, parameter tables, forcing links) from the standardized inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4a — SUMMA-specific preprocessing\n",
    "symfluence.managers['model'].preprocess_models()\n",
    "print(\"Model-specific preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4b — Run the Model\n",
    "\n",
    "Executes the point-scale SUMMA simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4b — Run SUMMA\n",
    "print(f\"Running {config.model.hydrological_model} for point-scale simulation...\")\n",
    "symfluence.managers['model'].run_models()\n",
    "print(\"Point-scale model run complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4c — Verification\n",
    "\n",
    "Print where SUMMA inputs and run outputs were written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4c — Verify SUMMA outputs\n",
    "\n",
    "domain_dir = config.system.data_dir / f\"domain_{config.domain.name}\"\n",
    "summa_in   = domain_dir / \"forcing\" / \"SUMMA_input\"\n",
    "results    = domain_dir / \"simulations\" / config.domain.experiment_id / \"SUMMA\"\n",
    "\n",
    "print(f\"SUMMA input dir: {summa_in if summa_in.exists() else '(not found)'}\")\n",
    "print(f\"Results dir:     {results if results.exists() else '(not found)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4d — SWE Evaluation (uncalibrated)\n",
    "\n",
    "Visualize the uncalibrated SWE simulation against SNOTEL observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4d — SWE evaluation (uncalibrated)\n",
    "from IPython.display import Image, display\n",
    "\n",
    "plot_paths = symfluence.managers['reporting'].visualize_summa_outputs(\n",
    "    experiment_id=config.domain.experiment_id\n",
    ")\n",
    "\n",
    "if 'scalarSWE' in plot_paths:\n",
    "    swe_plot = Path(plot_paths['scalarSWE'])\n",
    "    print(f\"SWE evaluation plot: {swe_plot}\")\n",
    "    display(Image(filename=str(swe_plot)))\n",
    "else:\n",
    "    print(\"scalarSWE plot not found. Available plots:\")\n",
    "    for var, path in plot_paths.items():\n",
    "        print(f\"  - {var}: {path}\")\n",
    "\n",
    "print(\"\\nUncalibrated SWE evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 — Calibration (Differential Evolution)\n",
    "\n",
    "We calibrate SUMMA snow parameters against SNOTEL SWE observations using **Differential Evolution (DE)**.\n",
    "\n",
    "### Why DE instead of DDS?\n",
    "\n",
    "- **DDS** (Dynamically Dimensioned Search) starts from a single initial point. If that point produces NaN values or model failures, the entire calibration can fail immediately.\n",
    "- **DE** is a generation-based algorithm that evaluates an entire population each iteration. It can tolerate individual model failures within a generation and still converge.\n",
    "\n",
    "### Calibration parameters\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `tempCritRain` | Temperature threshold for rain vs snow |\n",
    "| `tempRangeTimestep` | Temperature range for mixed precipitation |\n",
    "| `frozenPrecipMultip` | Frozen precipitation undercatch correction |\n",
    "| `albedoMax` | Maximum snow albedo |\n",
    "| `albedoMinWinter` | Minimum winter snow albedo |\n",
    "| `albedoDecayRate` | Rate of snow albedo decay |\n",
    "| `k_soil` | Hydraulic conductivity of soil |\n",
    "| `vGn_n` | van Genuchten n parameter (pore-size distribution) |\n",
    "| `theta_sat` | Saturated water content |\n",
    "\n",
    "> **Note:** The configuration was set in Step 1a. If you need to change calibration settings, you must re-create the config and re-initialize SYMFLUENCE (all configs are frozen after creation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5a — Run calibration (DE + KGE)\n",
    "\n",
    "print(f\"Algorithm:          {config.optimization.algorithm}\")\n",
    "print(f\"Metric:             {config.optimization.metric}\")\n",
    "print(f\"Target:             {config.optimization.target}\")\n",
    "print(f\"Iterations:         {config.optimization.iterations}\")\n",
    "print(f\"Population size:    {config.optimization.population_size}\")\n",
    "print(f\"Calibration period: {config.domain.calibration_period}\")\n",
    "print()\n",
    "\n",
    "results_file = symfluence.managers['optimization'].calibrate_model()\n",
    "print(f\"\\nCalibration results file: {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5b — Post-calibration visualization\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "plot_paths = symfluence.managers['reporting'].visualize_calibration_results(\n",
    "    experiment_id=config.domain.experiment_id\n",
    ")\n",
    "\n",
    "for plot_name, plot_path in plot_paths.items():\n",
    "    print(f\"\\n{plot_name}:\")\n",
    "    display(Image(filename=str(plot_path)))\n",
    "\n",
    "print(\"\\nPost-calibration visualization complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (symfluence-root)",
   "language": "python",
   "name": "symfluence-root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
